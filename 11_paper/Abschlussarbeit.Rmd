---
title: "Title"
subtitle: "Subtitle"
type: "Type of Paper"
author: "Name"
discipline: "Study Path"
date: "today"
studid: "Matriculation Number"
supervisor: "Prof. Dr. Christoph Hanck"
secondsupervisor: "Prof. Dr. Andreas Behr"
ssemester: "1"
estdegree_emester: "Summer Term 2019"
deadline: "Deadline"
output:
  pdf_document:
    keep_tex: yes
    template: template.tex
    fig_caption: yes
    citation_package: biblatex
    number_sections: true
toc: true
lot: true
lof: true
graphics: true
biblio-title: References
fontsize: 11.5pt
geometry: lmargin=5cm,rmargin=2.5cm,tmargin=2.5cm,bmargin=2.5cm
biblio-files: references.bib
classoption: a4paper
---

<!-- % Template Version 1.2 -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
An issue of substantial interest in time series analysis is, whether there exists any meaningful equilibrium relationship between two or more time series variables. Various hypothesis tests have been suggested for testing this so-called cointegration relationship, with the null hypothesis of no cointegration. Their local power, however, relies mostly on a specific nuisance parameter, namely the squared long-run correlations of error terms driving the variables. This may lead to inconclusive results, as one test may reject the null hypothesis, while others accept. The detection of cointegration relationships among time series variables is therefore complicated. Furthermore, the decision for an applicable test poses a challenge for the practitioner. 

An approach for resolving this issue might be a combination of the different tests. Bayer and Hanck suggest a method for providing meta tests, with high power for all forms of the nuisance parameter [@Bayerhanck2009]. Their approach is based on Fisher's Chi-squared test [@Fisher1925]. It can be shown, that this provides an unambiguous test decision. 

So far, there exists a Stata module for computing the above-mentioned non-cointegration test. However, there is no implementation in R yet. Therefore, the objective of this work was the development of the R package **bayerhanck**, to implement the eponymous test. In section 2, the theoretical background of the combined non-cointegration test will be explained further. Next, the structure of the associated R package and its functions will be illustrated. Finally, the performance of this approach will be evaluated.



# Theory of Non-Cointegration Tests
As mentioned above, @Bayerhanck2009 point out, that there is no uniformly most powerful test for cointegration. This is due to the fact, that all common tests are affected by the nuisance parameter $R^2$, where $R^2$ is defined as the squared correlation of $\pmb{\nu}_{1t}$ with $\nu_{2t}$.^[$R^2:= \omega_{12}^{'} \pmb{\Omega}_{11}^{-1}\omega_{12} / \omega_{22}$] Furthermore, @pesavento2004 has shown that for varying $R^2$ different tests are more powerful. Hence it is not clear which test should be applied to the time series data. 

To resolve these issues, Bayer and Hanck propose a combination of various cointegration tests. To derive the properties of the underlying tests and their combination, Bayer and Hanck use the following model by @pesavento2004:

\begin{align}
\label{eq:regressor}
\triangle \pmb{x}_t = \pmb{\tau}_1 &+  \pmb{\nu}_{1t}\\
\label{eq:y}
y_t  = \left(\mu_2 - \pmb{\theta}' \pmb{\mu}_1 \right) &+ \left(\tau_2 - \pmb{\theta}' \pmb{\tau}_1 \right)t + \pmb{\theta}' x_t + u_t \nonumber
\end{align}

where $u_t = \rho u_{t-1} + v_{2t}$.

Equation \eqref{eq:regressor} describes the regressors, whereas equation \eqref{eq:y} describes the possible cointgeration vector. The observed sample can be denoted as $\pmb{z}_0, \ldots , \pmb{z}_T$, where $\pmb{z}_t = (\pmb{x}_t^{'}, y_t)$. 

Under the following two assumptions and $\rho = 1$ the $\mathcal{H}_0$ hypothesis holds for the vector $\pmb{z}_t$: 

\begin{itemize}
  \item[] Assumption 1: 
  \begin{itemize}
    \item[] $\{ \pmb{\nu}_t \}$ satisfied a Functional CLT, i.e. $\displaystyle T^{-1/2} \sum_{t = 1}^{[\cdot T]} \pmb{\nu}_t \Rightarrow \pmb{\Omega}^{1/2} \pmb{W}(\cdot)$, i.e $\pmb{z}_t$ is $I(1)$  
  \end{itemize}
  \item[] Assumption 2:
  \begin{itemize}
    \item[] There are no cointegrated relationships among the variables in $\pmb{x}_t$
  \end{itemize}
\end{itemize}

Two essential properties of this model are: firstly, the local power of the underlying tests only depends on the parameters $c := T(\rho -1)$ and $R^2$. Secondly, even asymptotically these statistics are only weakly correlated under $\mathcal{H}_0$. [@gregory_mixed_2004] 

With these central findings it may be possible to achieve a more robust test by combining individual underlying tests. Bayer and Hanck utilise the cointegration tests of Engel and Granger [@Englegranger1987], Johansen [@Johansen1988], Boswijk [@Boswijk1994] and Banerjee [@Banerjee1998] as their underlying tests.

Each of these tests has a slightly different approach to testing for a possible cointegration relationship. The Engle-Granger Test tests the null of non cointegration against the alternative hypothesis of at least one cointegration relationship. For this, a simple two-step procedure is used. Firstly, the residuals $\hat{u}_t$ of a regression of the dependent variable on the independent variable is computed. Then, the first differences of these residuals are regressed on the lagged residuals. The test statistic for the Engle-Granger Test is then the $t$-statistic $t^{ADF}_\gamma$ on $\gamma$ in the aforesaid second regression $\triangle \hat{u}_t = \gamma \hat{u}_{t-1} + \sum_{p = 1}^{P-1}\nu_p \triangle \hat{u}_{t-p} +\epsilon_t$. [@Englegranger1987]

As opposed to the Engle-Granger Test, @Johansen1988's cointegration test is able to test for $h$ multiple cointegration relationships. The null hypothesis is $h = 0$. To implement this test, a vector error correction model (VECM)

\begin{align}
\triangle {\bf z}_t = {\bf \Pi z}_{t-1} + \sum^{P-1}_{p=1}\pmb{\Gamma}_p \triangle{\bf z}_{t-p} + {\bf d}_t + \pmb{\epsilon}_t
\end{align}

needs to estimated first. Bayer and Hanck employ the test statstic $\lambda_{\max} (h) = - T \ ln(1 - \hat{\pi}_1)$, where $\hat{\pi}_1$ denotes the largest solution of $|\pi \pmb{S}_{11} - \pmb{S}_{10} \pmb{S}_{00}^{-1} \pmb{S}_{01}|= 0$.^[$\pmb{S}_{ij}$ denotes the moment matrices of redcued rank regression residuals.]

The cointegration tests of @Boswijk1994 and @Banerjee1998 are based on error correction model tests. Boswijk's test statistic $\hat{F}$ is the Wald statistic for $\mathcal{H}_0: (\varphi_0, \pmb{\phi}_1^{'}) = 0$, whereas Banerjee _et al._'s test statistic is the $t_{\gamma}^{ECR}$ ratio for $\mathcal{H}_0 : \varphi_0 = 0$ of the error correction model. To derive the error correction model, the usual least squares estimate is used: $\triangle y_t = d_t + \pi_{0x}^{'} \triangle \pmb{x}_t + \varphi_0 y_{t-1} + \pmb{\varphi}_{1}^{'} \pmb{x}_{t-1} + \sum_{P = 1}^{P} \left( \pmb{\pi}_{px}^{'} \triangle  \pmb{x}_{t-p} + \pi_{py} \triangle y_{t - p} \right) + \epsilon_t$. 

From now on, $t_i$ denotes the test statistic of the underlying test $i$. If the this test rejects for large (small) values, take $\xi_i := t_i (-\xi_i = t_i)$. To combine these tests into a more powerful joint non cointegration test, Bayer and Hanck use an aggregator employed by @Fisher1925's Chi-squared test. The test statistic $\tilde{\chi}_{\mathcal{I}}^2$ is than defined as:

\begin{align}
  \label{eq:bayer-hanck}
  \tilde{\chi}_{\mathcal{I}}^{2} := -2 \sum_{i \in \mathcal{I}} \ln(p_i),
\end{align}

where $\mathcal{I}$ is the index set of the $\xi_i$.

The test statistic has the property, that the distribution under the $\mathcal{H}_0$ hypothesis converges to a random variable $\mathcal{F}_{\mathcal{I}}$ $\left(\tilde{\chi}_{\mathcal{I}}^{2} \rightarrow_{d} \mathcal{F}_{\mathcal{I}} \right)$. This guarantees, that the statistic has a well-defined asymptotic null distribution. According to @Bayerhanck2009, this null distribution is nuisance parameter free and only depends on the amount of tests combined and the selection of tests combined. Consequently, the joint $\mathcal{H_0}$ hypotheses can be simulated. 

Under the alternative hypotheses $\mathcal{H}_1$ the test statistic probability converges to $\tilde{\chi}_{\mathcal{I}}^{2} \rightarrow_P  \infty$, which means that the test statistic is consistent if at least one of the underlying tests is consistent.



# Implementation of the Package **bayerhanck** in R
The package consists of four functions for the underlying tests, as well as the function for the combined test. Furthermore, the cumulative distribution function of the null hypothesis can be plotted. The package features will be illustrated by using real data from the lutkepohl dataset.

## Implementation of the underlying Tests
The underlying tests can be carried out by their eponymous commands, namely `englegranger()`, `johansen()`, `banerjee()` and `boswijk()`. The former two partly rely on already implemented functions from the packages **urca** and **tsDyn**. Due to the absence of associated functions for the latter two, those had to be programmed manually. All functions take several arguments, from which only `formula` and `data` need to be filled out by the practitioner. Further arguments orientate themselves on the default values defined in the Stata implementation of the Bayer-Hanck Test. For the argument `lags`, which determines the number of lags to be included in the model, the default value is therefore set to 1. The argument `trend` describes the deterministic components of the model. The practitioner may choose from `none`, for no deterministics, `const`, for a (unrestricted) constant and `trend`, for a (unrestricted) constant plus (unrestricted) trend. The default value is set to `const`. Aside from the Johansen Test, all functions where programmed in such a way, that `trend = "none"` simply removes the intercept from all performed linear regressions in the functions. For `trend = "trend"` the intercept was maintained and a trend component was added with `seq_along()`. This generates a sequence, which usually takes the length of the dependent variable $y$. For `trend = "const"` the intercept was maintained and thus no changes were made to the linear regression formula. The trend selection for the Johansen Test will be explained later on. 

The functions of the underlying tests all return an object of classes `co.test` and `list`. The console also returns the value of the test statistic, as well as the name of the executed test. 

In accordance with the previously explained structure, the function `englegranger()` takes the form:

```{r eval = FALSE}
englegranger(formula, data, lags = 1, trend = "const")
```

The structure of this function is orientated towards the implementation of the Engle-Granger test in the aforesaid Stata module. Therefore, none of the various existing functions in R is used. Firstly, a linear regression is performed, according to the formula entered in `englegranger()`. Next, an augmented Dickey-Fuller test is applied to the residuals from this regression. For this, the function `ur.df()` from the **urca** package is used. The output value of the test statistic is then used as the test statistic of the Engle-Granger test.

The function for the Johansen test contains the additional argument `type`, which specifies if an maximum eigenvalue or a trace test should be conducted. Therefore, the options are either `trace` or `eigen`, with `eigen` being the default choice. The structure of the function takes the form:

```{r eval = FALSE}
johansen(formula, data, type = "eigen", lags = 1, trend = "const")
```

First of all, if the practitioner chooses `trend = "trend"`, it has to be renamed to `"both"`. This describes the required name for the following functions to correctly specify the requested deterministic component. Then, the function estimates a VECM by Johansen (MLE) method with `VECM` from the package **tsDyn**^[Compared to `ca.jo` from the **urca** package, the functions from **tsDyn** allow for the specification of an unrestricted constant and trend, which where used in the Stata Package for the Bayer-Hanck Test.]. A test of the cointegrating rank is conducted. For this, the function `rank.test()` from the same package is used. Here, the maximum eigenvalue is used as the output test statistic of `johansen()`.

The construction of the functions for the Banerjee and Boswijk test was almost identical. Therefore, both functions will be illustrated in a single step. The arguments for both functions are identical to those from `englegranger()`:

```{r eval = FALSE}
banerjee(formula, data, lags = 1, trend = "const")
boswijk(formula, data, lags = 1, trend = "const")

```

For the construction of these functions, first differences had to be taken of the dependent, as well as the independent variables. Furthermore, a matrix of the lagged values of all variables in first differences was constructed. Then, the procedure for the Banerjee and Boswijk test were implemented as outlined above. For calculating the test statistics, the coefficients, as well as the covanriance-matrix were extracted from the last linear regression model. For the Banerjee test, the coefficient of the first regressor has to be divided by the associated variance from the covariance-matrix. Here, one has to consider, that the position of said values changes, when `trend = "none"`, as the intercept is removed^[Due to the different order in the Stata output, this could have been ignored there, but poses a threat in R.]. 

For the test statistic of the Boswijk test, the vector of coefficients will firstly be matrix multiplied with the inverse covariance-matrix. Next, this product will then again be matrix multiplied with the vector of coefficients. 

## Implementation of the function `bayerhanck()`

@Bayerhanck2009's combined non-cointegration test can be carried out by the command `bayerhanck()`. It accepts all of the arguments of the previously explained underlying tests. Furthermore, it also contains the additional arguments `test`and `crit`, which can be passed to the function call. The function thus takes the form:

```{r eval = FALSE}
bayerhanck(formula, data, lags = 1, trend = "const", 
           test = "all", crit = 0.05)
```

The argument `test` determines which combination of the underlying tests should be executed. Here, the practitioner can choose between the default `all` (performs all of the aforementioned tests) or `eg-j`, which only carries out the Engel-Granger and Johansen test.

The argument `crit` sets the significance level of the test. The practitioner may choose between 0.01, 0.05 and 0.1, with the default 0.05.

After inserting the formula and the data, the selected underlying tests are called. The test statistics of each selected test are obtained and stored in a vector. Then, the p-values are calculated by using the corresponding empirical null distribution. This distribution depends on the number of variables, the trend specification and the combination of the underlying tests. Next, the p-values are logarithmised and summed. In accordance with equation \eqref{eq:bayer-hanck}, they are multiplied with -2 to obtain the test statistic. In the last step, the critical value is selected from the empirical null distribution, subject to the specified significance level.

## Implementation of the function `bayerhanck_1()`

The obtained values from the previously presented function `bayerhanck()` use the critical values provided by the technical appendix from @Bayerhanck2009. As an extension to this function, the function `bayerhanck_1()` has been developed. It contains additional null distributions, calculated with a specifically conducted simulation. Analogous to the basic version, it takes the form:

```{r eval = FALSE}
bayerhanck_1(formula, data, lags = 1, trend = "const", 
           test = "all", crit = 0.05)
```

Due to the availability of the simulated distributions unter the $\mathcal{H}_0$, the function is now capable of calculating and printing p-values. In its current implementation, the function stayed with the initial options for the argument `test`, `eg-j` and `all`. At this point in time, the amount of data is already large and further increases with each additional test combination by about 7 Megabyte. This is due to the fact, that for each combination of regressors and trendtypes, a simulated $\mathcal{H}_0$ distribution with 25,000 values is needed. In the current state, the package with the function `bayerhanck_1` has a size of about 16 Megabyte with only two test options. However, in order to reduce the amount of data and allow for further test combinations, the simulated distribution might be reduced to 10,000 values per combination of regressors and trendtypes, once the package is further developed.


## Implementation of the function `plot()` and `plot.bh.test()` 

```{r eval = FALSE}
plot(object, theme = "dark")
plot.bh.test(object, theme = "dark")
```

In addition, we have also implemented a plot function that displays the simulated cumulative distribution under the $\mathcal{H}_0$ and the Bayer-Hanck statistics. The plot function can be executed either with the short command `plot()` or with the command `plot.bh.test()`.

The function call needs the output object of the `bayerhanck()` function, therefore the object must have the class `bh.test`. The user may use the argument `theme` to choose between the option `dark` for a dark theme or `light` for a light theme. 

The command passes the Bayerhanck statistics, the number of regressors and the trend type. Thus the function knows which null distribution should be plotted. This data is then passed to the `ggplot2` command and the function `ggplot::stat_ecdf()` is used to plot the zero distribution. Furthermore the bayerhanck statistic is indicated with an x-intercept line.


## Implementation of own version of a `summary()` function  

```{r eval = FALSE}
summary(object)
```

To present the results of the Bayer-Hanck non cointegration test in a reasonable way we have also created our own `summary()` function. 

The function only needs the created object from the `bayerhanck()` or `bayerhanck_1()` function of the class `bh.test`.

The function includes and displays the originally entered formula of the VARS, the number of lags and the trend specification. Furthermore the matrix with the test statistics and the p-values for the underlying test is displayed. In the last third the bayerhanck statistic is displayed with the corresponding critical value  




# Conclusion

\newpage












