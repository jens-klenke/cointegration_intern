---
title: "Title"
subtitle: "Subtitle"
type: "Type of Paper"
author: "Name"
discipline: "Study Path"
date: "today"
studid: "Matriculation Number"
supervisor: "Prof. Dr. Christoph Hanck"
secondsupervisor: "Prof. Dr. Andreas Behr"
ssemester: "1"
estdegree_emester: "Summer Term 2019"
deadline: "Deadline"
output:
  pdf_document:
    keep_tex: yes
    template: template.tex
    fig_caption: yes
    citation_package: biblatex
    number_sections: true
toc: true
lot: true
lof: true
graphics: true
biblio-title: References
fontsize: 11.5pt
geometry: lmargin=5cm,rmargin=2.5cm,tmargin=2.5cm,bmargin=2.5cm
biblio-files: references.bib
classoption: a4paper
---

<!-- % Template Version 1.2 -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
An issue of substantial interest in time series analysis is, whether there exists any meaningful equilibrium relationship between two or more time series variables. Various hypothesis tests have been suggested for testing this so-called cointegration relationship, with the null hypothesis of no cointegration. Their local power, however, relies mostly on a specific nuisance parameter, namely the squared long-run correlations of error terms driving the variables. This may lead to inconclusive results, as one test may reject the null hypothesis, while others accept. The detection of cointegration relationships among time series variables is therefore complicated. Furthermore, the decision for an applicable test poses a challenge for the practitioner. 

An approach for resolving this issue might be a combination of the different tests. Bayer and Hanck suggest a method for providing meta tests, with high power for all forms of the nuisance parameter [@Bayerhanck2009]. Their approach is based on Fisher's Chi-squared test [@Fisher1925]. It can be shown, that this provides an unambiguous test decision. 

So far, there exists a Stata module for computing the above-mentioned non-cointegration test. However, there is no implementation in R yet. Therefore, the objective of this work was the development of the R package **bayerhanck**, to implement the eponymous test. In section 2, the theoretical background of the combined non-cointegration test will be explained further. Next, the structure of the associated R package and its functions will be illustrated. Finally, the performance of this approach will be evaluated.



# Theory of Non-Cointegration Tests
As mentioned above, @Bayerhanck2009 point out, that there is no uniformly most powerful test for cointegration. This is due to the fact, that all common tests are affected by the nuisance parameter $R^2$, where $R^2$ is defined as the squared correlation of $\pmb{\nu}_{1t}$ with $\nu_{2t}$.^[$R^2:= \omega_{12}^{'} \pmb{\Omega}_{11}^{-1}\omega_{12} / \omega_{22}$] Furthermore, @pesavento2004 has shown that for varying $R^2$ different tests are more powerful. Hence it is not clear which test should be applied to the time series data. 

To resolve these issues, Bayer and Hanck propose a combination of various cointegration tests. To derive the properties of the underlying tests and their combination, Bayer and Hanck use the following model by @pesavento2004:

\begin{align}
\label{eq:regressor}
\triangle \pmb{x}_t = \pmb{\tau}_1 &+  \pmb{\nu}_{1t}\\
\label{eq:y}
y_t  = \left(\mu_2 - \pmb{\theta}' \pmb{\mu}_1 \right) &+ \left(\tau_2 - \pmb{\theta}' \pmb{\tau}_1 \right)t + \pmb{\theta}' x_t + u_t \nonumber
\end{align}

where $u_t = \rho u_{t-1} + v_{2t}$.

Equation \eqref{eq:regressor} describes the regressors, whereas equation \eqref{eq:y} describes the possible cointgeration vector. The observed sample can be denoted as $\pmb{z}_0, \ldots , \pmb{z}_T$, where $\pmb{z}_t = (\pmb{x}_t^{'}, y_t)$. 

Under the following two assumptions and $\rho = 1$ the $\mathcal{H}_0$ hypothesis holds for the vector $\pmb{z}_t$: 

\begin{itemize}
  \item[] Assumption 1: 
  \begin{itemize}
    \item[] $\{ \pmb{\nu}_t \}$ satisfied a Functional CLT, i.e. $\displaystyle T^{-1/2} \sum_{t = 1}^{[\cdot T]} \pmb{\nu}_t \Rightarrow \pmb{\Omega}^{1/2} \pmb{W}(\cdot)$, i.e $\pmb{z}_t$ is $I(1)$  
  \end{itemize}
  \item[] Assumption 2:
  \begin{itemize}
    \item[] There are no cointegrated relationships among the variables in $\pmb{x}_t$
  \end{itemize}
\end{itemize}

Two essential properties of this model are: firstly, the local power of the underlying tests only depends on the parameters $c := T(\rho -1)$ and $R^2$. Secondly, even asymptotically these statistics are only weakly correlated under $\mathcal{H}_0$. [@gregory_mixed_2004] 

With these central findings it may be possible to achieve a more robust test by combining individual underlying tests. Bayer and Hanck utilise the cointegration tests of Engel and Granger [@Englegranger1987], Johansen [@Johansen1988], Boswijk [@Boswijk1994] and Banerjee [@Banerjee1998] as their underlying tests.

Each of these tests has a slightly different approach to testing for a possible cointegration relationship. The Engle-Granger Test tests the null of non cointegration against the alternative hypothesis of at least one cointegration relationship. For this, a simple two-step procedure is used. Firstly, the residuals $\hat{u}_t$ of a regression of the dependent variable on the independent variable is computed. Then, the first differences of these residuals are regressed on the lagged residuals. The test statistic for the Engle-Granger Test is then the $t$-statistic $t^{ADF}_\gamma$ on $\gamma$ in the aforesaid second regression $\triangle \hat{u}_t = \gamma \hat{u}_{t-1} + \sum_{p = 1}^{P-1}\nu_p \triangle \hat{u}_{t-p} +\epsilon_t$. [@Englegranger1987]

As opposed to the Engle-Granger Test, @Johansen1988's cointegration test is able to test for $h$ multiple cointegration relationships. The null hypothesis is $h = 0$. To implement this test, a vector error correction model 

\begin{align}
\triangle {\bf z}_t = {\bf \Pi z}_{t-1} + \sum^{P-1}_{p=1}\pmb{\Gamma}_p \triangle{\bf z}_{t-p} + {\bf d}_t + \pmb{\epsilon}_t
\end{align}

needs to estimated first. Bayer and Hanck employ the test statstic $\lambda_{\max} (h) = - T \ ln(1 - \hat{\pi}_1)$, where $\hat{\pi}_1$ denotes the largest solution of $|\pi \pmb{S}_{11} - \pmb{S}_{10} \pmb{S}_{00}^{-1} \pmb{S}_{01}|= 0$.^[$\pmb{S}_{ij}$ denotes the moment matrices of redcued rank regression residuals.]

The cointegration tests of @Boswijk1994 and @Banerjee1998 are based on error correction model tests. Boswijk's test statistic $\hat{F}$ is the Wald statistic for $\mathcal{H}_0: (\varphi_0, \pmb{\phi}_1^{'}) = 0$, whereas Banerjee _et al._'s test statistic is the $t_{\gamma}^{ECR}$ ratio for $\mathcal{H}_0 : \varphi_0 = 0$ of the error correction model. To derive the error correction model, the usual least squares estimate is used: $\triangle y_t = d_t + \pi_{0x}^{'} \triangle \pmb{x}_t + \varphi_0 y_{t-1} + \pmb{\varphi}_{1}^{'} \pmb{x}_{t-1} + \sum_{P = 1}^{P} \left( \pmb{\pi}_{px}^{'} \triangle  \pmb{x}_{t-p} + \pi_{py} \triangle y_{t - p} \right) + \epsilon_t$. 

From now on, $t_i$ denotes the test statistic of the underlying test $i$. If the this test rejects for large (small) values, take $\xi_i := t_i (-\xi_i = t_i)$. To combine these tests into a more powerful joint non cointegration test, Bayer and Hanck use an aggregator employed by @Fisher1925's Chi-squared test. The test statistic $\tilde{\chi}_{\mathcal{I}}^2$ is than defined as:

\begin{align}
  \label{eq:bayer-hanck}
  \tilde{\chi}_{\mathcal{I}}^{2} := -2 \sum_{i \in \mathcal{I}} \ln(p_i),
\end{align}

where $\mathcal{I}$ is the index set of the $\xi_i$.

The test statistic has the property, that the distribution under the $\mathcal{H}_0$ hypothesis converges to a random variable $\mathcal{F}_{\mathcal{I}}$ $\left(\tilde{\chi}_{\mathcal{I}}^{2} \rightarrow_{d} \mathcal{F}_{\mathcal{I}} \right)$. This guarantees, that the statistic has a well-defined asymptotic null distribution. According to @Bayerhanck2009, this null distribution is nuisance parameter free and only depends on the amount of tests combined and the selection of tests combined. Consequently, the joint $\mathcal{H_0}$ hypotheses can be simulated. 

Under the alternative hypotheses $\mathcal{H}_1$ the test statistic probability converges to $\tilde{\chi}_{\mathcal{I}}^{2} \rightarrow_P  \infty$, which means that the test statistic is consistent if at least one of the underlying tests is consistent.



# Implementation of the Package **bayerhanck** in R
The package consists of four functions for the underlying tests, as well as the function for the combined test. Furthermore, the cumulative distribution function of the null hypothesis can be plotted. The package features will be illustrated by using real data from the lutkepohl dataset.

## Implementation of the underlying Tests
The underlying tests can be carried out by their eponymous commands, namely `englegranger()`, `johansen()`, `banerjee()` and `boswijk()`. The former two partly rely on already implemented functions from the packages **urca** and **tsDyn**. Due to the absence of associated functions for the latter two, those had to be programmed manually. All functions take several arguments, from which only `formula` and `data` need to be filled out by the practitioner. Further arguments orientate themselves on the default values defined in the Stata implementation of the Bayer-Hanck Test. For the argument `lags`, which determines the number of lags to be included in the model, the default value is therefore set to 1. The argument `trend` describes the deterministic components of the model. The practitioner may choose from `none`, for no deterministics, `const`, for a (unrestricted) constant and `trend`, for a (unrestricted) constant plus (unrestricted) trend. The default value is set to `const`. Aside from the Johansen Test, all functions where programmed in such a way, that `trend = "none"` simply removes the intercept from all performed linear regressions in the functions. For `trend = "trend"` the intercept was maintained and a trend component was added with `seq_along()`. This generates a sequence, which usually takes the length of the dependent variable $y$. For `trend = "const"` the intercept was maintained and thus no changes were made to the linear regression formula. The trend selection for the Johansen Test will be explained later on. 

The functions of the underlying tests all return an object of classes `co.test` and `list`. The console also returns the value of the test statistic, as well as the name of the executed test. 

In accordance with the previously explained structure, the function `englegranger()` takes the form:

```{r eval = FALSE}
englegranger(formula, data, lags = 1, trend = "const")
```

The structure of this function is orientated towards the implementation of the Engle-Granger test in the aforesaid Stata module. Therefore, none of the various existing functions in R is used. Firstly, a linear regression is performed, according to the formula entered in `englegranger()`. Next, an augmented Dickey-Fuller test is applied to the residuals from this regression. For this, the function `ur.df()` from the **urca** package is used. The output value of the test statistic is then used as the test statistic of the Engle-Granger test.

The function for the Johansen test contains the additional argument `type`, which specifies if an maximum eigenvalue or a trace test should be conducted. Therefore, the options are either `trace` or `eigen`, with `eigen` being the default choice. The structure of the function takes the form:
```{r eval = FALSE}
johansen(formula, data, type = "eigen", lags = 1, trend = "const")
```

Firstly, the function estimates a VECM by Johansen (MLE) method with `VECM` from the package **tsDyn**. Then, a test of the cointegrating rank is conducted. For this, the function `rank.test()` from the same package is used. In comparison with similar functions `rank.test()` possesses the advantage of the possibility to select the unrestricted constant or trend. Here, the maximum eigenvalue is used as the output test statistic of `johansen()`.

The construction of the functions for the Banerjee and Boswijk test was almost identical. Therefore, both functions will be illustrated in a single step. The arguments of both functions are identical to those from `englegranger()`:

```{r eval = FALSE}
banerjee(formula, data, lags = 1, trend = "const")
boswijk(formula, data, lags = 1, trend = "const")

```

For the construction of those functions, first differences had to be taken of the dependent, as well as the independent variables. Furthermore, a matrix of the lagged values of all variables in first differences was constructed.

\textcolor{red}{Weitere Beschreibung Banerjee/Boswijk (Jens)}

From this linear Regression the coefficients, as well as the covariance-matrix is extracted. Now, the test statistic of both tests can be calculated. For the test statistic of the Banerjee test, the coefficient of \textcolor{red}{Position} has to be divided by the associated entry in the covariance-matrix. For the test statistic of the Boswijk test, all coefficients will firstly be matrix multiplied with the inverse covariance-matrix. Next, this product will then again be matrix multiplied with the coefficients. 

## Implementation of the function `bayerhanck()`

```{r eval = FALSE}
bayerhanck(formula, data, lags = 1, trend = "const", 
           test = "all", crit = 0.05)
```

The combined non-cointegration test can be carried out by the command `bayerhanck()` which accepts all the arguments of the underlying tests. Also the additional arguments `test` and `crit` can be passt to the function call. The argument `test` etermines which set of underlying tests should be executed. Here users can choose between the default `all` (performs all the above tests), or `eg-j`, which only carries out the Engel-Granger and Johansen test. For the argument `crit`, which sets the significance level of the test, the user may choose between `0.10`, the default `0.05`, or `0.01`, corresponding with 10 \%, 5 \%, or 1\% significance. 

After the selected underlying tests have been performed, the test statistics of each test are obtained and the p-values are calculated by using the corresponding empirical null distribution which depends on the number of variables in the VAR and the trend specification.

In the following step, the individual p-values are logarithmized , summed and then taken with minus 2 times, in accorance with equation \eqref{eq:bayer-hanck}, to obtain the test statistics. In the last step, the critical value is selected based on the specified significance level from the empirical null distribution of the combined test, which depends on the number of regressors and the trend type.


# Conclusion

\newpage












