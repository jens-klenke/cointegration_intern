---
title: 'P-Approximation'
author: 'Jens Klenke and Janine Langerbein'
subtitle: 'Seminar in Econometrics'
type: "Term Paper"
discipline: "VWL M.Sc."
date: "today"
studid: "3071594"
supervisor: "Christoph Hanck"
secondsupervisor: "NA"
semester: "6"
estdegree_emester: "Winter Term 2020"
deadline: "Jan. 17th 2020"
output:
  pdf_document:
    keep_tex: yes
    template: template.tex
    fig_caption: yes
    citation_package: biblatex
    number_sections: true
toc: true
lot: true
lof: true
graphics: true
biblio-title: References
linkcolor: black
urlcolor: black
citecolor: black
colorlinks: true
font: Times New Roman
fontsize: 12pt
geometry: lmargin = 5cm,rmargin = 2.5cm,tmargin = 2.5cm,bmargin = 2.5cm
biblio-files: references.bib
classoption: a4paper
---

<!-- % Template Version 1.1 -->

<!-- Git version -->

```{r , include=FALSE}
#Sys.setlocale(locale = "English_United States.1252") ## English US Windows
#knitr::opts_chunk$set(echo = TRUE)

#options(kableExtra.latex.load_packages = FALSE)

## packages
source(here::here('01_code/packages/packages.R'))

# load metrics
load(here::here('09_simulation_and_approximation-cdf/server_results.RData'))

# pre rendered plot
load(file = here::here('09_simulation_and_approximation-cdf/01_paper/paper_plots.RData'))

best_5_table_paper <- function(data){
    data%>%
        dplyr::slice_min(RMSE_cor_0.2, n = 5)%>%
        dplyr::select(-c(formula, expo, model))
}
```

# Introduction
Meta tests have been shown to be a powerful tool when testing for the null of non-cointegration. The distribution of their test statistic, however, is mostly not available in closed form. This might pose difficulties when implementing the meta tests in econometric software packages, as one has to include the full null distribution for each combination of the underlying tests. Software package size limitations are therefore quickly exceeded. 

In this paper we propose supervised Machine Learning Algorithms to approximate the p-values of the meta test by @Bayerhanck_2012 which tests for the null of non-cointegration. This approach might reduce the size of associated software packages considerably. The algorithms are trained on simulated data for various specifications of the aforementioned test. 

\textcolor{red}{Ergebnis der Models (1-2 Sätze)}

\textcolor{red}{Inhalt Paper}

# Bayer Hanck Test
The choice as to which of the available cointegration tests to use is a recurrent issue in econometric time series analysis. @Bayerhanck_2012 propose powerful meta tests which provide unambiguous test decisions. They combine several residual- and system-based tests in the manner of Fisher's [-@Fisher_1932] Chi-squared test. 

Bayer and Hanck build their paper on previous work from @Pesavento_2004, who defines the underlying model as $z'_t = [x'_t, y_t]$, with $x_t$ being an $n_1 \times 1$ vector and $y_t$ a scalar, which displays the cointegration relation. They can be written as

\begin{align}
\Delta x_t &= \tau_1 + v_{1t} \label{eq:11} \\
y_t &= (\mu_2 - \gamma' \mu_1) + (\tau_2 - \gamma' \tau_1) t + \gamma' x_t + u_t, \label{eq:12} \\
u_t &= \rho u_{t-1} + v_{2t}. \label{eq:13}
\end{align}


$\Delta x_t$ presents the regressor dynamics. $\mu_1$, $\mu_2$, $\tau_1$ and $\tau_2$ are the deterministic parts of the model. They are subject to the following restrictions: (i) $\mu_2 - \gamma' \mu_1$ and $\tau = 0$ which translates to no deterministics, (ii) $\tau = 0$ which corresponds to a constant in the cointegrating vector, (iii) $\tau_2 - \gamma' \tau_1 = 0$, a constant plus trend.

$v_t = [v'_{1t} v_{2t}]'$ with $\Omega$ the long-run covariance matrix of $v_t$. For derivation of $v_t$ see @Pesavento_2004. Pesavento shows that {$v_t$} satisfies an FCLT, i.e. $T^{-1/2} \sum^{[T \cdot]}_{t=1} v_t \Rightarrow \Omega^{1/2} W(\cdot)$. It is further assumed that the $x_t$ are not cointegrated.

It clearly follows from \eqref{eq:13} that $z_t$ is cointegrated if $\rho < 1$. Hence the null hypothesis of no cointegration is $H_0: p = 1$. 
Furthermore, Pesavento introduces two other parameters. First, $\text{R}^2$ measures the squared correlation of $v_{1t}$ and $v_{2t}$. It can be interpreted as the influence of the right-hand side variables in \eqref{eq:12}. It ranks between zero and one. When there is no long-run correlation between those variables and the errors from the cointegration regression, $\text{R}^2$ equals zero. Secondly, the number of lags is approximated by a finite number $k$.

\textcolor{red}{Assumptions (BH S. 84)?}

Bayer and Hanck's [-@Bayerhanck_2012] meta test considers the test statistics of up to four stand-alone tests. Namely, these are the tests of @Englegranger_1987, @Johansen_1988, @Boswijk_1994 and @Banerjee_1998. For the sake of brevity the detailed derivation of the underlying tests has been deliberately omitted here. 

@Englegranger_1987 propose a two-step procedure to test the null hypothesis of no cointegration against the alternative of at least one cointegrating vector. First, the long-run relationship between $y_t$ and $\mathbf{x}_t$ is estimated by least squares regression. The obtained residuals $\hat{u}_t$ are then tested for a unit root. For this, Engle and Granger suggest the use of the $t$-statistic $t^{\text{ADF}}_\gamma$ in the Augmented Dickey-Fuller (ADF) regression:
\begin{equation}
\Delta \hat{u}_t = \gamma \hat{u}_{t-1} + \sum^{k}_{i=1} \pi_i \Delta \hat{u}_{t-i} + \varepsilon_t.
\label{eq:2}
\end{equation}
The rejection of a unit root points to a cointegration relationship. 

Johansen's [-@Johansen_1988] maximum eigenvalue test is a system-based test that allows for several cointegration relationships. Take the vector error correction model (VECM)^[Due to practical reasons we omit the derivation of the VECM which is presumed to be known.]
\begin{equation}
\Delta \mathbf{z}_t = \mathbf{\Pi z}_{t-1} + \sum^{k}_{i = 1} \mathbf{\Gamma}_p \Delta \mathbf{z}_{t-p} + \mathbf{d}_t + \mathbf{\varepsilon}_t.
\label{eq:3}
\end{equation}
We base this test on the test statistic $\lambda_{\text{max}} = -T \ln(1 - \hat{\lambda}_t)$. \textcolor{red}{$\pi$-Teil von BH?}

The third and fourth test considered are error correction-based. Both estimate the equation
\begin{equation}
\Delta y_t = d_t + \pi'_{0x} \Delta x_t + \varphi_0 y_{t-1} + \varphi'_1 x_{t-1} + \sum^P_{p=1} (\pi'_{px} \Delta x_{t-p} + \pi_{py} \Delta y_{t-p})
\label{eq:4}
\end{equation}
by ordinary least squares (OLS). @Banerjee_1998 then test the null of non-cointegration by applying a t-test on $\varphi_0$, i.e. $\mathcal{H}_0 : \varphi_0 = 0$ \textcolor{red}{?}. @Boswijk_1994 uses the Wald statistic for testing $\mathcal{H}_0 : (\varphi_0, \phi'_1)' = 0$.

To combine the results from the underlying tests Bayer and Hanck draw upon Fisher's combined probability test [@Fisher_1932]. It merges the tests using the formula

\begin{equation}
\tilde{\chi}^2_{\mathcal{I}} := -2 \sum_{i \in \mathcal{I}} \ln{(p_i)}. 
\label{eq:5}
\end{equation}

Let $t_i$ be the $i^{th}$ test statistic. If test $i$ rejects for large values, take $\xi_i := t_i$. If test $i$ rejects for small values, take $-\xi_i := t_i$. With $\Xi_i(x) := \text{Pr}_{\mathcal{H_0}}(\xi_i \geq x)$ the p-value of the $i^{th}$ test is $p_i := \Xi_i(\xi_i)$.

@Fisher_1932 shows that under the assumption of independence the null distribution of $\tilde{\chi}^2_{\mathcal{I}}$ follows a chi-squared distribution with $2\mathcal{I}$ degrees of freedom. If this assumption is violated the null distribution is less evident. Here, the latter case occurs, as the $\xi_i$ are not independent. The $\tilde{\chi}^2_{\mathcal{I}}$, however, have well-defined asymptotic null distributions $F_{\mathcal{F_I}}$, as $\tilde{\chi}^2_{\mathcal{I}} \rightarrow_d \mathcal{F_I}$ under $\mathcal{H}_0$ if $T \rightarrow \infty$, with $\mathcal{F_I}$ some random variable. It is therefore feasible to simulate the joint null distribution of the $\xi_i$ to obtain the distribution $F_{\mathcal{F_I}}$ of \eqref{eq:5}. The $F_{\mathcal{F_I}}$ depend on which and how many tests are combined. The distributions of the $\xi_i$ depend on $K-1$ and the deterministic case.


# Simulation
In this section, we describe the simulation of the null distribution of the Bayer Hanck meta test. The objective is to obtain data for training machine learning algorithms on approximating the p-values of the aforementioned test. In consideration of the different forms of the meta test we generate six data sets. These vary according to the specific combinations of the underlying tests and also account for the above-mentioned restrictions on the deterministic parts of the model.

The following approach relies largely on previous work by @Pesavento_2004. For calculating the Bayer Hanck test statistic we require the p-values of the underlying tests. For this, we simulate their null distributions. It can be shown that asymptotically these are functions of standard Brownian motions. Here, the latter are constructed by step functions using Gaussian random walk of size $N = 1000$. The number of repetitions is set to 1,000,000. Furthermore, we consider $\text{R}^2 \in \{0, 0.05, 0.1, ..., 0.95\}$, the maximum number of lags $K = 11$ and $c = 0$^[Since we solely aim at simulating the distribution of the null of no cointegration we will not consider any further values of $c$ here.] \textcolor{red}{(c mal definieren)}.

From the mass of test statistics we build the \ac{CDF} of each underlying test and calculate the respective p-values. These are inserted into \eqref{eq:4} to eventually obtain the Bayer Hanck test statistics. Analogous to the previous approach, we deduce the associated null distribution and the p-values. 


# Models
We now use the generated data for training machine learning algorithms on predicting the approximated empirical \ac{CDF} of the Bayer Hanck test. We work with the values of the test statistic and the number of lags $k$ as predictors. As it is our objective to describe the null distribution with a less memory-intensive model we will only consider linear methods. For the same objective we compare the models according to their in-sample \ac{RMSE}. The threat of overfitting is thus of no particular relevance here. For this reason, and to reduce computation time, we use no cross-validation.

As the empirical \ac{CDF} is typically known to be curved in an S-shape we skip the classic linear regression in favor of a more flexible model. We stay with least squares regression, but try various combinations of polynomial functions and interaction terms of the aforementioned regressors. The search for the best model is carried out via brute-force.

## Data Pre-Processing
One approach for improving a model's predictive ability is the pre-processing of the training data. Some models, like linear regression, react sensitively to certain characteristics of the predictor or response data. Those characteristics include, inter alia, distributional skewness and outliers and there exist several methods to lower their potentially bad impact on the model's performance. 

Since we simulated our training data under the null of non-cointegration we expect the distribution of the test statistic to be rather right skewed. \textcolor{red}{Plot} also reveals it to have a long right tail. If we train our regression model on this raw data it can possibly have difficulties predicting from high values of the test statistic. 

One of the aforementioned methods to deal with such issues are power transforms. One might decide freely which transformation to apply. Alternatively, there exist statistical methods to determine an appropriate transformation. A well-known family of transformations to un-skew data is the Box-Cox transformation [@Boxcox_1964]. They aim at transforming the data so that it closely resembles the normal distribution. The exact transformation depends on the parameter $\lambda$, whose optimal value can be empirically estimated: 

\begin{equation}
y^{(\lambda)} =
    \begin{cases}
    \frac{y^{\lambda} - 1}{\lambda}, & \lambda \neq 0 \\
    \log{(y)}, & \lambda = 0
    \end{cases}
\label{eq:6}
\end{equation}

It is visible from \eqref{eq:6} that @Boxcox_1964 developed these transformations for the dependent variable. @Kuhn_2013, however, report that it proves as effective for transforming individual regressors. We estimate lambda for the values of the test statistics of the Bayer-Hanck test and transform them according to \eqref{eq:5}. This forces their distribution into a more symmetric form. 

Since the response variable consists of our p-values, which were simulated under the null hypothesis, it follows a uniform distribution and is already symmetric. A transformation would therefore not bring any apparent advantage. However, we still add a Box-Cox transformed and a logarithmised version of the response variable to see if it benefits the prediction.

We also include various variations of the actual categorical variable $k$. It is firstly decomposed into dummy variables and secondly recode as a numeric, so that various transformations can be performed.

## Polynomial Regression
Due to the reasons given above we restrict ourselves to linear models. The empirical \ac{CDF}, which we aim to predict, is known to have a curved shape. For this reason, a simple linear regression model is very unlikely to provide a satisfactory fit to the data. We are in need of a more flexible model to predict the response as accurately as possible.

Polynomial Regression extends the classic linear regression model by fitting a polynomial equation of arbitrary order to the data. A polynomial regression with $n$ degrees thus takes the form

\begin{equation}
    y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + ... + \beta_n x_i^n + \varepsilon_i,
\label{eq:7}
\end{equation}

where $\varepsilon_i$ is the error term. \textcolor{red}{Quelle?}

Here, we calculate orthogonal polynomials of the test statistic of the Bayer-Hanck Test, considering up to 15 degrees. We estimate the parameters with OLS. To potentially increase the predictive performance of our model we also add interaction terms and different transformations of the regressor $k$. \textcolor{red}{Appendix} lists all calculated models. Since there is no need to prevent overfitting we expect higher order polynomials to perform best, as they are highly flexible. These polynomials, however, tend to show a wiggly behaviour at the boundaries. This makes extrapolation beyond the limits of our simulated data a risky endeavour. We will address and fix this issue later on. 

## \ac{Lasso}  
As mentioned above our polynomial regression models are likely to perform best with higher order polynomials. With each added polynomial, however, we increase the complexity of our model and potentially add redundant regressors. Although, still, overfitting plays no major role here, we generally prefer sparser models in case of equal results. One way to deal with this is the use of variable selection methods. A well-known example of such methods is the \ac{Lasso}. 

The lasso estimate is defined as 

\begin{equation}
    \hat{\beta}^{\text{lasso}} = \argmin_{\beta} \sum^N_{i=1} \left( y_i - \beta_0 - \sum^p_{j=1} x_{ij} \beta_j \right)^2
    \text{s.t.} \sum^p_{j=1} |\beta_j | \leq t, 
\label{eq:8}
\end{equation}

where the first term describes the residual sum of squares, subject to a term known as L1 penalty. In its Lagrangian form this can be rewritten as

\begin{equation}
    \hat{\beta}^{\text{lasso}} = \argmin_{\beta} \frac{1}{2} \sum^N_{i=1} \left( y_i - \beta_0 - \sum^p_{j=1} x_{ij} \beta_j \right)^2 + 
    \lambda \sum^p_{j=1} |\beta_j |
\label{eq:9}
\end{equation}

$\lambda$ is a tuning parameter which defines the degree of regularisation. The lasso penalty shrinks the coefficients and, for $\lambda$ sufficiently large, can set them to zero. The value of $\lambda$ is data dependent and is usually estimated with cross-validation. \textcolor{red}{ausführlicher? Quelle?}

We plan on fitting a LASSO model to polynomials of grade 15. We consider the same transformations and interaction terms as in earlier steps. We therefore fit a total of \textcolor{red}{Anzahl} models.

## Other Regression Models 
We also considered various other regression models. For different reasons they were not too suitable for our use case. Conventional non-linear methods, like Generalized Additive Models or Multivariate Adaptive Regression Splines, might have provided a decent prediction. However, the fitted models take up more memory space than the aforementioned linear methods. For the same reason refrain from using tree based methods. In addition, the latter tend to perform poorly with such a small amount of regressors. Given these limitations, we decided to stick solely with linear regression models.


# Model Evaluation
We estimate all models for two different combinations of the underlying tests. Namely, these are a combination of the Engle-Granger and Johansen test (EJ) and a combination of all four underlying tests (all). Furthermore, we estimate one model per specification of the model deterministics. Altogether, this results in a total of six different models. 

## RMSE comparison
To measure the performance of our regression models we calculate their in-sample RMSE. This is an indication of how far the residuals of the models are from zero, with lower values preferable. We calculate the RMSE for predictions on the full distribution, as well as predictions on the lower tail ($p \leq 0.2$), as it is more important for the test decision of the Bayer-Hanck test. We also add a corrected version of the RMSE, cRMSE, where predictions are limited to [0, 1].

Table \ref{tab:all_1} lists all variations of the RMSE for the calculated polynomial regression models. It becomes apparent that a combination of higher order polynomials, dummy variables and interaction terms indeed achieves superior results compared to simpler models. For all variations of the RMSE the best models require a polynomial of minimum grade 12. That was to be expected, considering we are optimising an in-sample fit. The transformation of the response variable only seems to play a minor role in prediction accuracy. Interestingly, there are no major differences in model selection depending on the variation of the RMSE used. Table \ref{tab:5_best_all_1} lists the five best models for each case and test type. 

For the above-mentioned reasons we choose the final models according to the cRMSE on the left tail of the distribution of the p-values. \textcolor{red}{Grafik mit den 6 final models}. It is apparent that the functional forms look very similar over all cases, mostly using the highest order polynomial available^[We are well aware of the fact that this represents a corner solution. Since we are optimising the models on the in-sample RMSE, however, we could continue adding higher order polynomials forever to improve the fit. As we are already tweaking on the fifth decimal place we decided to not further pursue this procedure.]. Furthermore, five out of six models use the Box-Cox transformed response variable. 

## Correction for high values of the test statistic

As described in [chapter 3](#Simulation) the data set used for training the models was simulated under the null hypothesis of no cointegration. It should be evident that for this reason most values of the test statistic will be comparatively small. Even after its transformation the distribution of the test statistic has a longer right tail, i.e. there exist few high values. When using the models within a software package, as originally intended, it is likely that they will face input values located on the far right of the central part of the distribution. It cannot be ruled out that the models will fail to make sensible predictions for such values of the test statistic. 

\ref{fig:fig_3} shows the prediction of the final models for all underlying tests on a sequence from 1 to 100, representing possible values of the test statistic. Surprisingly, in the majority of cases the models perform well, with the prediction line taking the expected shape. In two cases, however, the predicted values rise again, taking values not equal to zero. More precisely, this occurs for the model with no deterministics (Case = 1) and $k = 3$ and $k = 4$, respectively.

\ref{fig:fig_4} shows the same behaviour for the data with Engle-Granger and Johansen as underlying tests for all combinations of cases and $k$. Above a certain value of the test statistic the predicted values sharply increase, converging \textcolor{red}{(?)} against 1. It should be noted that this upper boundary is enforced by our build in correction for predicted values outside the interval [0, 1]. Without this intervention the predicted values would probably rise even further. If we predict on an extended sequence with no correction, the prediction line most likely oscillates above a certain value. 

It cannot be clearly established why the models' prediction behaves this way. Oscillation at the edges of an interval is a common problem in polynomial interpolation, especially when using polynomials of high order. Additionally, the distribution of the test statistic in the training data may have made matters worse. It must also be considered that we chose our models according to their predictive performance on the lower tail of the distribution, possibly neglecting the predictive performance on the upper tail. If this incident is not rectified the models will be unable to provide reliable test decisions, as they tend to falsely not reject the null hypothesis at high values of the test statistic. The approach is therefore prone to type II errors.

\textcolor{red}{(Jens)}

Technically, the correction for each model was done as follow. For each test configuration and case, the final model -- 6 models in total -- was used to approximate the $p$-values for the test statistic ranging from $1$ to $100$ in $0.1$ jumps. Subsequently, the smallest test statistic was selected where the lower bound ($1e-12$) for the $p$-value was predicted by the model. Thus, a p-value of $< 1e-12$ is automatically predicted for all test statics greater than this critical value in the package.


# Package

## Algorithmic implementation

Not part of this chapter on package implementation will be the calculation of the underlying tests and Bayerhanck statistics, as these are already available. The approximation of the $p$-value is implemented by means of a separate function named `get_p_value`, which takes the arguments `bh.test`, `trendtype`, `test.type`, and `k`. With `bh.test` specifying the test statistic, `trendtype` the case, `test.type` which underlying test were used and `k` the number of regressors. The first step within the function is to check whether the calculated test statistic is greater than the critical value, given the configuration of the test and the parameters. When this is the case, a deterministic $p$-value of $<1e-12$ is always predicted and the rest of the approximation process is skipped. However, if this is not the case -- i.e., the test statistic is less than the critical value -- the actual approximation of the function is now performed. 

All required data and models are organized in the tibble `models`, which contains the `test.type`, the `trendtype`, the `models`, the $\lambda$ value for the box-cox transformation for the test statistic and the $p$-value, and the `critical value`. In order to perform the approximation, we first need the $\lambda$ parameters for the two transformation of the test statistic and the $p$-value for the corresponding configuration of the test performed. In the next step the corresponding model is selected. Before we can perform the estimation we have to merge the passed variables, e.g. `bh.test` and `k`, into a data frame and then additionally include them as a box-cox transformation.

Now we can do the p-value approximation using the appropriate model. If the response variable was box-cox transformed in the model, the variable is transformed back in the next step. The last step in the algorithm is to keep the estimated p-value within the theoretical limits of 0 and 1. If a p-value of 0 or a negative value is predicted, it is transformed to $< 1e-12$ as in the correction before. For values above 1, it is set to $9.9999e-1$.   





<!-- end of main part -->

\pagebreak

\pagenumbering{Roman}
\setcounter{page}{3}
\addcontentsline{toc}{section}{References}
\printbibliography[title = References]
\cleardoublepage

\begin{refsection}
\nocite{R-base}
\nocite{R-stargazer}
\nocite{R-stringr}
\nocite{R-tidyr}
\nocite{R-dplyr}
\nocite{R-glmnet}
\nocite{R-class}
\nocite{R-MASS}
\nocite{R-plm}
\nocite{R-leaps}
\nocite{R-caret}
\nocite{R-tree}
\nocite{R-gbm}
\nocite{R-plotmo}
\nocite{R-pls}
\nocite{R-splines}
\nocite{R-tictoc}
\nocite{R-plotly}
\nocite{R-inspectdf}
\nocite{R-rpart}
\nocite{R-rpart.plot}
\nocite{R-stargazer}
\nocite{R-knitr}
\nocite{R-purrr}
\nocite{R-randomForest}
\nocite{R-rstudioapi}





\nocite{R-Studio}

\printbibliography[title = Software-References]
\addcontentsline{toc}{section}{Software-References}
\end{refsection}


<!---
--------------------------------------------------------------------------------
------------- Appendix ---------------------------------------------------------
--------------------------------------------------------------------------------
-->

\cleardoublepage
\appendix
\setcounter{table}{0}
\setcounter{figure}{0}
\renewcommand{\thetable}{A\arabic{table}}
\renewcommand{\thefigure}{A\arabic{figure}}

\newgeometry{top = 2.5cm, left = 2.5cm, right = 2cm, bottom = 2cm}

# Appendices

Table \ref{tab:func_form} list the different functional forms of the polynomial regression we tested. In total we investigated $21$ different forms and for each of these forms we investigated the polynomial in the range from $3$ to $13$. As equations with many polynomials are getting very long we will use a short-hand notation. For example the first equation in Table \ref{tab:func_form} for a polynomial of $3$ is in short-hand notation

\begin{align}
    p = c + \poly\left( \bc(t), 3 \right)
\end{align}

and represents

\begin{align}
    p = c + \gamma_{1,1} t + \gamma_{1,2} t^2 + \gamma_{1,1} t^3 .
\end{align}

\begin{table}
	\centering
	\caption{Description of all tested functional forms for polynomial regression. All functional forms were tested for a maximum polynomial degree from 3 to 13. The shorthand notation was used for the description.}
	\label{tab:func_form}	 
	\begin{tabular}{rlc}
		Number & Functional form & Range of $\gamma$ \\
		\toprule
		$1$ & $p = c + \poly\left( \bc(t), \gamma \right) $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\ 
		$2$ & $p = c + \poly\left( \bc(t), \gamma \right) + k $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\
		$3$ & $p = c + \poly\left( \bc(t), \gamma \right) * k $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\
		$4$ & $p = c + \poly\left( \bc(t), \gamma \right) + \log(k) $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\
		$5$ & $p = c + \poly\left( \bc(t), \gamma \right) * \log(k) $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\
		$6$ & $p = c + \poly\left( \bc(t), \gamma \right) + k\_d $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\
		$7$ & $p = c + \poly\left( \bc(t), \gamma \right) * k\_d $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\ %[0.5em]
		\midrule
		$8$ & $\log(p) = c + \poly\left( \bc(t), \gamma \right) $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\ 
		$9$ & $\log(p) = c + \poly\left( \bc(t), \gamma \right) + k $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\
		$10$ & $\log(p) = c + \poly\left( \bc(t), \gamma \right) * k $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\
		$11$ & $\log(p) = c + \poly\left( \bc(t), \gamma \right) + \log(k) $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\
		$12$ & $\log(p) = c + \poly\left( \bc(t), \gamma \right) * \log(k) $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\
		$13$ & $\log(p) = c + \poly\left( \bc(t), \gamma \right) + k\_d $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\
		$14$ & $\log(p) = c + \poly\left( \bc(t), \gamma \right) * k\_d $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\	
		\midrule
		$15$ & $\bc(p) = c + \poly\left( \bc(t), \gamma \right) $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\ 
		$16$ & $\bc(p) = c + \poly\left( \bc(t), \gamma \right) + k $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\
		$17$ & $\bc(p) = c + \poly\left( \bc(t), \gamma \right) * k $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\
		$18$ & $\bc(p) = c + \poly\left( \bc(t), \gamma \right) + \log(k) $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\
		$19$ & $\bc(p) = c + \poly\left( \bc(t), \gamma \right) * \log(k) $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\
		$20$ & $\bc(p) = c + \poly\left( \bc(t), \gamma \right) + k\_d $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\
		$21$ & $\bc(p) = c + \poly\left( \bc(t), \gamma \right) * k\_d $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\	
		\bottomrule
	\end{tabular}
\end{table}
\FloatBarrier

## Results for the $p$-approximation of the Bayer-Hanck Test with all underyling Tests

### Metrics of the 5 Best Models

```{r 5_best_all_1, echo = FALSE, warning = FALSE, message = FALSE}

table_all_case_1 %>%
    best_5_table_paper() %>%
    dplyr::mutate_if(is.numeric, funs(as.character(format(., scientific = TRUE, digits = 3)))) %>%
    knitr::kable(booktabs = TRUE, linesep = "", col.names = c('Model', 'RMSE', 'cRMSE', 'RMSE', 'cRMSE' ), 
             row.names = TRUE, escape = FALSE,
              caption= '\\label{tab:best_all_1} The five best models, based on the cRMSE for the lower tail of the distribution, for the first case (no constant, no trend) and all underlying tests included. The RMSE and cRMSE were calculated over the whole distribution and over the lower tail of the distribution. The cRMSE reflects the RMSE after correcting for values ranging between 0 and 1.', align = 'lrrrr') %>%
    column_spec(3:6, width = "2cm") %>%
  kable_styling(latex_options = c("striped", "hold_position"), font_size = 10) %>%
  add_header_above(c("", "", "Full Distribution" = 2, "Lower Tail ($p \\\\leq 0.2$)" = 2),  bold = TRUE, escape = FALSE)
```

```{r 5_best_all_2, echo = FALSE, warning = FALSE, message = FALSE}

table_all_case_2 %>%
    best_5_table_paper() %>%
    dplyr::mutate_if(is.numeric, funs(as.character(format(., scientific = TRUE, digits = 3)))) %>%
    knitr::kable(booktabs = TRUE, linesep = "", col.names = c('Model', 'RMSE', 'cRMSE', 'RMSE', 'cRMSE' ), 
             row.names = TRUE, escape = FALSE,
              caption= '\\label{tab:best_all_2} The five best models, based on the cRMSE for the lower tail of the distribution, for the second case (with constant, no trend) and all underlying tests included. The RMSE and cRMSE were calculated over the whole distribution and over the lower tail of the distribution. The cRMSE reflects the RMSE after correcting for values ranging between 0 and 1.', align = 'lrrrr') %>%
    column_spec(3:6, width = "2cm") %>%
  kable_styling(latex_options = c("striped", "hold_position"), font_size = 10) %>%
  add_header_above(c("", "", "Full Distribution" = 2, "Lower Tail ($p \\\\leq 0.2$)" = 2),  bold = TRUE, escape = FALSE)
```

```{r 5_best_all_3, echo = FALSE, warning = FALSE, message = FALSE}

table_all_case_3 %>%
    best_5_table_paper() %>%
    dplyr::mutate_if(is.numeric, funs(as.character(format(., scientific = TRUE, digits = 3)))) %>%
    knitr::kable(booktabs = TRUE, linesep = "", col.names = c('Model', 'RMSE', 'cRMSE', 'RMSE', 'cRMSE' ), 
             row.names = TRUE, escape = FALSE,
              caption= '\\label{tab:best_all_3} The five best models, based on the cRMSE for the lower tail of the distribution, for the third case (with constant and trend) and all underlying tests included. The RMSE and cRMSE were calculated over the whole distribution and over the lower tail of the distribution. The cRMSE reflects the RMSE after correcting for values ranging between 0 and 1.', align = 'lrrrr') %>%
    column_spec(3:6, width = "2cm") %>%
  kable_styling(latex_options = c("striped", "hold_position"), font_size = 10) %>%
  add_header_above(c("", "", "Full Distribution" = 2, "Lower Tail ($p \\\\leq 0.2$)" = 2),  bold = TRUE, escape = FALSE)
```

\FloatBarrier
### Metrics of all Models
```{r all_1, echo = FALSE, warning = FALSE, message = FALSE}
table_all_case_1 %>%
    dplyr::select(-c(formula, expo, model)) %>%
    dplyr::mutate_if(is.numeric, funs(as.character(format(., scientific = TRUE, digits = 3)))) %>%
    knitr::kable(booktabs = TRUE, linesep = "", col.names = c('Model', 'RMSE', 'cRMSE', 'RMSE', 'cRMSE' ), 
             row.names = TRUE, escape = FALSE,
              caption= '\\label{tab:all_1} Performance of the models for the first case and all underlying tests included. The RMSE and cRMSE were calculated over the whole distribution and over the lower tail of the distribution. The cRMSE reflects the RMSE after correcting for values ranging between 0 and 1.', align = 'lrrrr', longtable = TRUE) %>%
    column_spec(3:6, width = "2cm") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), font_size = 10) %>%
  add_header_above(c("", "", "Full Distribution" = 2, "Lower Tail ($p \\\\leq 0.2$)" = 2),  bold = TRUE, escape = FALSE)
```


```{r all_2, echo = FALSE, warning = FALSE, message = FALSE}
table_all_case_2 %>%
    dplyr::select(-c(formula, expo, model)) %>%
    dplyr::mutate_if(is.numeric, funs(as.character(format(., scientific = TRUE, digits = 3)))) %>%
    knitr::kable(booktabs = TRUE, linesep = "", col.names = c('Model', 'RMSE', 'cRMSE', 'RMSE', 'cRMSE' ), 
             row.names = TRUE, escape = FALSE,
              caption= '\\label{tab:all_2} Performance of the models for the second case and all underlying tests included. The RMSE and cRMSE were calculated over the whole distribution and over the lower tail of the distribution. The cRMSE reflects the RMSE after correcting for values ranging between 0 and 1.', align = 'lrrrr', longtable = TRUE) %>%
    column_spec(3:6, width = "2cm") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), font_size = 10) %>%
  add_header_above(c("", "", "Full Distribution" = 2, "Lower Tail ($p \\\\leq 0.2$)" = 2),  bold = TRUE, escape = FALSE)
```

```{r all_3, echo = FALSE, warning = FALSE, message = FALSE}
table_all_case_3 %>%
    dplyr::select(-c(formula, expo, model)) %>%
    dplyr::mutate_if(is.numeric, funs(as.character(format(., scientific = TRUE, digits = 3)))) %>%
    knitr::kable(booktabs = TRUE, linesep = "", col.names = c('Model', 'RMSE', 'cRMSE', 'RMSE', 'cRMSE' ), 
             row.names = TRUE, escape = FALSE,
              caption= '\\label{tab:all_3} Performance of the models for the third case and all underlying tests included. The RMSE and cRMSE were calculated over the whole distribution and over the lower tail of the distribution. The cRMSE reflects the RMSE after correcting for values ranging between 0 and 1.', align = 'lrrrr', longtable = TRUE) %>%
    column_spec(3:6, width = "2cm") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), font_size = 10) %>%
  add_header_above(c("", "", "Full Distribution" = 2, "Lower Tail ($p \\\\leq 0.2$)" = 2),  bold = TRUE, escape = FALSE)
```

## Results for the $p$-approximation of the Bayer-Hanck Test with Engle-Granger and Johansen as underlying tests

### Metrics of the 5 Best Models

```{r 5_best_e_j_1, echo = FALSE, warning = FALSE, message = FALSE}
table_E_J_case_1 %>%
    best_5_table_paper() %>%
    dplyr::mutate_if(is.numeric, funs(as.character(format(., scientific = TRUE, digits = 3)))) %>%
    knitr::kable(booktabs = TRUE, linesep = "", col.names = c('Model', 'RMSE', 'cRMSE', 'RMSE', 'cRMSE' ), 
             row.names = TRUE, escape = FALSE,
              caption= '\\label{tab:best_e_j_1} The five best models, based on the cRMSE for the lower tail of the distribution, for the first case (no constant, no trend) with Engle-Granger and Johansen as underlying tests. The RMSE and cRMSE were calculated over the whole distribution and over the lower tail of the distribution. The cRMSE reflects the RMSE after correcting for values ranging between 0 and 1.', align = 'lrrrr') %>%
    column_spec(3:6, width = "2cm") %>%
  kable_styling(latex_options = c("striped", "hold_position"), font_size = 10) %>%
  add_header_above(c("", "", "Full Distribution" = 2, "Lower Tail ($p \\\\leq 0.2$)" = 2),  bold = TRUE, escape = FALSE)
```

```{r 5_best_e_j_2, echo = FALSE, warning = FALSE, message = FALSE}
table_E_J_case_2 %>%
    best_5_table_paper() %>%
    dplyr::mutate_if(is.numeric, funs(as.character(format(., scientific = TRUE, digits = 3)))) %>%
    knitr::kable(booktabs = TRUE, linesep = "", col.names = c('Model', 'RMSE', 'cRMSE', 'RMSE', 'cRMSE' ), 
             row.names = TRUE, escape = FALSE,
              caption= '\\label{tab:best_e_j_2} The five best models, based on the cRMSE for the lower tail of the distribution, for the second case (with constant, no trend) with Engle-Granger and Johansen as underlying tests. The RMSE and cRMSE were calculated over the whole distribution and over the lower tail of the distribution. The cRMSE reflects the RMSE after correcting for values ranging between 0 and 1.', align = 'lrrrr') %>%
    column_spec(3:6, width = "2cm") %>%
  kable_styling(latex_options = c("striped", "hold_position"), font_size = 10) %>%
  add_header_above(c("", "", "Full Distribution" = 2, "Lower Tail ($p \\\\leq 0.2$)" = 2),  bold = TRUE, escape = FALSE)
```

```{r 5_best_e_j_3, echo = FALSE, warning = FALSE, message = FALSE}
table_E_J_case_3 %>%
    best_5_table_paper() %>%
    dplyr::mutate_if(is.numeric, funs(as.character(format(., scientific = TRUE, digits = 3)))) %>%
    knitr::kable(booktabs = TRUE, linesep = "", col.names = c('Model', 'RMSE', 'cRMSE', 'RMSE', 'cRMSE' ), 
             row.names = TRUE, escape = FALSE,
              caption= '\\label{tab:best_e_j_3} The five best models, based on the cRMSE for the lower tail of the distribution, for the third case (with constant and trend) with Engle-Granger and Johansen as underlying tests. The RMSE and cRMSE were calculated over the whole distribution and over the lower tail of the distribution. The cRMSE reflects the RMSE after correcting for values ranging between 0 and 1.', align = 'lrrrr') %>%
    column_spec(3:6, width = "2cm") %>%
  kable_styling(latex_options = c("striped", "hold_position"), font_size = 10) %>%
  add_header_above(c("", "", "Full Distribution" = 2, "Lower Tail ($p \\\\leq 0.2$)" = 2),  bold = TRUE, escape = FALSE)
```

\FloatBarrier

### Metrics of all Models
```{r e_j_1, echo = FALSE, warning = FALSE, message = FALSE}
table_E_J_case_1 %>%
    dplyr::select(-c(formula, expo, model)) %>%
    dplyr::mutate_if(is.numeric, funs(as.character(format(., scientific = TRUE, digits = 3)))) %>%
    knitr::kable(booktabs = TRUE, linesep = "", col.names = c('Model', 'RMSE', 'cRMSE', 'RMSE', 'cRMSE' ), 
             row.names = TRUE, escape = FALSE,
              caption= '\\label{tab:e_j_1} Performance of the models for the first case with Engle-Granger and Johansen as underlying tests. The RMSE and cRMSE were calculated over the whole distribution and over the lower tail of the distribution. The cRMSE reflects the RMSE after correcting for values ranging between 0 and 1.', align = 'lrrrr', longtable = TRUE) %>%
    column_spec(3:6, width = "2cm") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), font_size = 10) %>%
  add_header_above(c("", "", "Full Distribution" = 2, "Lower Tail ($p \\\\leq 0.2$)" = 2),  bold = TRUE, escape = FALSE)
```

```{r e_j_2, echo = FALSE, warning = FALSE, message = FALSE}
table_E_J_case_2 %>%
    dplyr::select(-c(formula, expo, model)) %>%
    dplyr::mutate_if(is.numeric, funs(as.character(format(., scientific = TRUE, digits = 3)))) %>%
    knitr::kable(booktabs = TRUE, linesep = "", col.names = c('Model', 'RMSE', 'cRMSE', 'RMSE', 'cRMSE' ), 
             row.names = TRUE, escape = FALSE,
              caption= '\\label{tab:e_j_2} Performance of the models for the second case with Engle-Granger and Johansen as underlying tests. The RMSE and cRMSE were calculated over the whole distribution and over the lower tail of the distribution. The cRMSE reflects the RMSE after correcting for values ranging between 0 and 1.', align = 'lrrrr', longtable = TRUE) %>%
    column_spec(3:6, width = "2cm") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), font_size = 10) %>%
  add_header_above(c("", "", "Full Distribution" = 2, "Lower Tail ($p \\\\leq 0.2$)" = 2),  bold = TRUE, escape = FALSE)
```

```{r e_j_3, echo = FALSE, warning = FALSE, message = FALSE}
table_E_J_case_3 %>%
    dplyr::select(-c(formula, expo, model)) %>%
    dplyr::mutate_if(is.numeric, funs(as.character(format(., scientific = TRUE, digits = 3)))) %>%
    knitr::kable(booktabs = TRUE, linesep = "", col.names = c('Model', 'RMSE', 'cRMSE', 'RMSE', 'cRMSE' ), 
             row.names = TRUE, escape = FALSE,
              caption= '\\label{tab:e_j_3} Performance of the models for the second case with Engle-Granger and Johansen as underlying tests. The RMSE and cRMSE were calculated over the whole distribution and over the lower tail of the distribution. The cRMSE reflects the RMSE after correcting for values ranging between 0 and 1.', align = 'lrrrr', longtable = TRUE) %>%
    column_spec(3:6, width = "2cm") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), font_size = 10) %>%
  add_header_above(c("", "", "Full Distribution" = 2, "Lower Tail ($p \\\\leq 0.2$)" = 2),  bold = TRUE, escape = FALSE)
```

\FloatBarrier





```{r , approx_sim-all, echo = FALSE, fig.cap = " \\label{fig:sim_approx_all} Simulated against approximated $p$-values over the whole distribution for all cases and all underlying tests. ", fig.height = 8}
p.sim_p.aprox_all
```

```{r , approx_sim-all_0.2, echo = FALSE, fig.cap = " \\label{fig:sim_approx_all_0.2} Simulated vs. approximated $p$-values for the lower tail of the distribution for all cases and all underlying test.", fig.height = 8}
p.sim_p.aprox_all_0.2
```

```{r , p_stat_all, echo = FALSE, fig.cap = " \\label{fig:fig_3} Corrected (blue) and uncorrected (red) $p$-value predictions for all cases and all underlying tests.", fig.height = 8}
plot_p_stat_all
```

```{r , p_stat_e_j, echo = FALSE, fig.cap = " \\label{fig:fig_4} Corrected (blue) and uncorrected (red) $p$-value predictions for all cases using Engle-Granger and Johansen as underlying tests.", fig.height = 8}
plot_p_stat_e_j
```



<!-- 
--------------------------------------------------------------------------------
------------- End of Appendix --------------------------------------------------
--------------------------------------------------------------------------------
--> 
\restoregeometry

\cleardoublepage


