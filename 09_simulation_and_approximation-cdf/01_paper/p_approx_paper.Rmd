---
title: 'P-Approximation'
author: 'Jens Klenke and Janine Langerbein'
subtitle: 'Seminar in Econometrics'
type: "Term Paper"
discipline: "VWL M.Sc."
date: "today"
studid: "3071594"
supervisor: "Christoph Hanck"
secondsupervisor: "NA"
semester: "6"
estdegree_emester: "Winter Term 2020"
deadline: "Jan. 17th 2020"
output:
  pdf_document:
    keep_tex: yes
    template: template.tex
    fig_caption: yes
    citation_package: biblatex
    number_sections: true
toc: true
lot: true
lof: true
graphics: true
biblio-title: References
linkcolor: black
urlcolor: black
citecolor: black
colorlinks: true
font: Times New Roman
fontsize: 12pt
geometry: lmargin = 5cm,rmargin = 2.5cm,tmargin = 2.5cm,bmargin = 2.5cm
biblio-files: references.bib
classoption: a4paper
---

<!-- % Template Version 1.1 -->

<!-- Git version -->

```{r , include=FALSE}
#Sys.setlocale(locale = "English_United States.1252") ## English US Windows
#knitr::opts_chunk$set(echo = TRUE)

#options(kableExtra.latex.load_packages = FALSE)

## packages
source(here::here('01_code/packages/packages.R'))

# load metrics
load(here::here('09_simulation_and_approximation-cdf/server_results.RData'))

# pre rendered plot
load(file = here::here('09_simulation_and_approximation-cdf/01_paper/paper_plots.RData'))

best_5_table_paper <- function(data){
    data%>%
        dplyr::slice_min(RMSE_cor_0.2, n = 5)%>%
        dplyr::select(-c(formula, expo, model))
}
```

# Introduction
Meta tests have been shown to be a powerful tool when testing for the null of non-cointegration. The distribution of their test statistic, however, is mostly not available in closed form. The calculation of the critical values, let alone p-values, is therefore a cumbersome procedure, as one has to simulate sufficient values of the test statistic under the null hypothesis to approximate their distribution. When implementing those meta tests in econometric software packages, one therefore has to include the full null distribution for each combination of the underlying tests. Software package size limitations are therefore quickly exceeded. 

One possible approach to this problem is to model the relationship between the p-values and the test statistic with a regression model. Instead of including the full null distribution in an econometric software package one only has to include this model. This might reduce the size of the software package considerably.

In this paper we approximate the p-values of the meta test by @Bayerhanck_2012 which tests for the null of non-cointegration with supervised Machine Learning Algorithms. We train our models on simulated values of the test statistic and the corresponding p-values for various specifications of the aforementioned test. Subsequently, these models will be included in the software package `bayerhanck` for the statistical programming language R. We find that this approach indeed reduces the size of the package significantly. Section 2 explains the theoretical background of the Bayer Hanck Test. We briefly introduce the underlying tests and describe the combination procedure of the meta test. Section 3 explains the simulation of the values of the test statistic and the calculation of the p-values. In Section 4 we describe the pre-processing of the data and the regression models used. Section 5 evaluates those models. For this, we compare their predictive performance by calculating several in-sample metrics. Furthermore, we discuss problems which have arisen and how to fix them. Finally, Section 6 outlines the implementation of the models in the aforementioned software package. Section 7 concludes.


# Bayer Hanck Test
The choice as to which of the available cointegration tests to use is an issue in econometric time series analysis. @Bayerhanck_2012 propose powerful meta tests which provide unambiguous test decisions. They combine several residual- and system-based tests in the manner of Fisher's [-@Fisher_1932] Chi-squared test. 

Bayer and Hanck build on previous work from @Pesavento_2004, who considers the model
\begin{align}
\Delta x_t &= \tau_1 + v_{1t} \label{eq:11} \\
y_t &= (\mu_2 - \theta' \mu_1) + (\tau_2 - \theta' \tau_1) t + \theta' x_t + u_t, \label{eq:12} \\
\text{with } u_t &= \rho u_{t-1} + v_{2t}. \label{eq:13}
\end{align}
\eqref{eq:11} represents the regressor dynamics, while \eqref{eq:12} describes the cointegrating relation. The observed sample $\mathbf{z}_0,..., \mathbf{z}_T$ can be written as $\mathbf{z}_t = (\mathbf{x}'_t, y_t)'$.  The deterministic part of the model is described by restrictions on $\mu'_1$, $\mu_2$, $\tau_1$ and $\tau_2$. Consider $\tau = [\tau'_1 \tau_2]'$. Then, these restrictions are (1) $\mu_2 - \theta' \mu_1$ and $\tau = 0$ which translates to no deterministics, (2) $\tau = 0$ which corresponds to a constant in the cointegrating vector, (3) $\tau_2 - \theta' \tau_1 = 0$, a constant plus trend. $v_t = [v'_{1t} v_{2t}]'$ with $\Omega$ the long-run covariance matrix of $v_t$. It can be shown that {$v_t$} satisfies an FCLT, i.e. $T^{-1/2} \sum^{[\cdot T]}_{t=1} v_t \Rightarrow \Omega^{1/2} W(\cdot)$. $W(\lambda)$ is a standard $(n_1 + 1) \times 1$ Brownian motion. It is also assumed that the variables in $x_t$ are not cointegrated. It further follows from \eqref{eq:13} that the vector $\mathbf{z}_t$ is cointegrated if $|\rho| < 1$. Hence the null hypothesis of no cointegration can be formulated as $H_0: p = 1$.

Furthermore, Pesavento introduces two other parameters. First, $\text{R}^2$ measures the squared correlation of $v_{1t}$ and $v_{2t}$. It can be interpreted as the influence of the right-hand side variables in \eqref{eq:12}. It ranks between zero and one. When there is no long-run correlation between those variables and the errors from the cointegration regression, $\text{R}^2$ equals zero. Secondly, the number of lags is approximated by a finite number $k$.

Bayer and Hanck's meta test enables the combination of up to four stand-alone tests. Namely, these are the tests of @Englegranger_1987, @Johansen_1988, @Boswijk_1994 and @Banerjee_1998. For the sake of brevity we will not present a detailed derivation of the underlying tests.

@Englegranger_1987 propose a two-step procedure to test the null hypothesis of no cointegration against the alternative of at least one cointegrating vector. First, the long-run relationship between $y_t$ and $\mathbf{x}_t$ is estimated by least squares regression. The obtained residuals $\hat{u}_t$ are then tested for a unit root. For this, Engle and Granger suggest the use of the $t$-statistic $t^{\text{ADF}}_\gamma$ in the Augmented Dickey-Fuller (ADF) regression:
\begin{equation}
\Delta \hat{u}_t = \gamma \hat{u}_{t-1} + \sum^{P-1}_{p=1} \nu_p \Delta \hat{u}_{t-p} + \varepsilon_t.
\label{eq:2}
\end{equation}
The rejection of a unit root points to a cointegration relationship. 

Johansen's [-@Johansen_1988] maximum eigenvalue test is a system-based test that allows testing for several cointegration relationships. Take the vector error correction model (VECM)
\begin{equation}
\Delta \mathbf{z}_t = \mathbf{\Pi z}_{t-1} + \sum^{P-1}_{p = 1} \mathbf{\Gamma}_p \Delta \mathbf{z}_{t-p} + \mathbf{d}_t + \mathbf{\varepsilon}_t.
\label{eq:3}
\end{equation}
We base this test on the test statistic $\lambda_{\text{max}}(h) = -T \ln(1 - \hat{\pi}_1)$. $\hat{\pi}_1$ is the largest solution to $[\pi \mathbf{S}_{11} - \mathbf{S}_{10} \mathbf{S}_{00}^{-1}\mathbf{S}_{01}] = 0$, with the $\mathbf{S}_{ij}$ being moment matrices of reduced rank regression residuals.

The third and fourth tests considered are error correction-based. Both estimate the equation
\begin{equation}
\Delta y_t = d_t + \pi'_{0x} \Delta x_t + \varphi_0 y_{t-1} + \varphi'_1 x_{t-1} + \sum^P_{p=1} (\pi'_{px} \Delta x_{t-p} + \pi_{py} \Delta y_{t-p}) + \varepsilon_t
\label{eq:4}
\end{equation}
by ordinary least squares (OLS). $P$ is chosen so that the $\varepsilon_t$ is approximately white noise. @Banerjee_1998 then test the null of non-cointegration by applying a t-test on $\varphi_0$, i.e. $\mathcal{H}_0 : \varphi_0 = 0$ \textcolor{red}{?}. @Boswijk_1994 uses the Wald statistic for testing $\mathcal{H}_0 : (\varphi_0, \phi'_1)' = 0$.

To combine the results from the underlying tests Bayer and Hanck draw upon Fisher's combined probability test [@Fisher_1932]. It merges the tests using the formula

\begin{equation}
\tilde{\chi}^2_{\mathcal{I}} := -2 \sum_{i \in \mathcal{I}} \ln{(p_i)},
\label{eq:5}
\end{equation}

where $t_i$ is the $i^{th}$ test statistic. If test $i$ rejects for large values, take $\xi_i := t_i$. If test $i$ rejects for small values, take $-\xi_i := t_i$. With $\Xi_i(x) := \text{Pr}_{\mathcal{H_0}}(\xi_i \geq x)$ the p-value of the $i^{th}$ test is $p_i := \Xi_i(\xi_i)$.

Fisher shows that under the assumption of independence the null distribution of $\tilde{\chi}^2_{\mathcal{I}}$ follows a chi-squared distribution with $2\mathcal{I}$ degrees of freedom. If this assumption is violated the null distribution is less evident. Here, the latter case occurs, as the $\xi_i$ are not independent. The $\tilde{\chi}^2_{\mathcal{I}}$, however, have well-defined asymptotic null distributions $F_{\mathcal{F_I}}$, as $\tilde{\chi}^2_{\mathcal{I}} \rightarrow_d \mathcal{F_I}$ under $\mathcal{H}_0$ if $T \rightarrow \infty$, with $\mathcal{F_I}$ some random variable. It is therefore feasible to simulate the joint null distribution of the $\xi_i$ to obtain the distribution $F_{\mathcal{F_I}}$ of \eqref{eq:5}. The $F_{\mathcal{F_I}}$ depend on number and type of the combined tests. The distributions of the $\xi_i$ depend on $K-1$ and the deterministic case.


# Simulation
As described in the previous section we can simulate the joint null distribution of the $\xi_i$ and by this allow conclusions on the null distribution of the Bayer Hanck test. In this section, we describe our simulation approach, which generates a large number of values of the test statistic of the Bayer Hanck test. As mentioned earlier it is a further objective to obtain sufficient data for subsequently training machine learning algorithms on approximating the p-values of the aforementioned test. 

In the later implementation phase of the package we plan on allowing to choose between two different combinations of the underlying tests. Namely, these will be a combination of the tests of Engle-Granger and Johansen and a combination of all possible underlying tests. We therefore calculate two different variables of the test statistic of the meta test. We also account for the above-mentioned restrictions on the deterministic parts of the model by generating three different data sets, each being based on the different associated data generating process. 

The following approach relies largely on previous work by @Pesavento_2004. It can be shown that the asymptotic null distributions of the underlying tests are functions of standard Brownian motions. We construct this by step functions using Gaussian random walk of size $N = 1000$. The number of repetitions is set to 1,000,000. Moreover, we consider $\text{R}^2 \in \{0, 0.05, 0.1, ..., 0.95\}$ and the maximum number of lags $K = 11$. Pesavento further introduces the local-to-unity parameter $c:= T(\rho-1)$. Clearly, for negative values of $c$ the variables are cointegrated, while for $c = 0$ there is no cointegration. Since we solely aim at simulating the distribution under the null hypothesis of no cointegration we will not consider other values different from $c = 0$ here. 

To calculate the $\tilde{\chi}^2_{\mathcal{I}}$ we require the p-values of the underlying tests. For this, we build the \ac{CDF} of each underlying test for the three different data sets and calculate the respective p-values. These are inserted into \eqref{eq:5} to eventually obtain the Bayer Hanck test statistic. Analogous to the previous approach, we deduce the associated null distribution and the p-values. 


# Models
We now use the simulated data for training machine learning algorithms on predicting the approximated empirical \ac{CDF} of the Bayer Hanck test. We work with the values of the test statistic and the number of lags $k$ as predictors. As it was said before, it is our objective to describe the null distribution with a less memory-intensive model. We will therefore only consider linear methods, as non-linear models typically take up too much memory to be suitable for this purpose. For the same reason we compare the models according to their in-sample \ac{RMSE}. The threat of overfitting is thus of no particular relevance here. For this reason, and to reduce computation time, we use no cross-validation.


## Data Pre-Processing
One approach for improving a model's predictive ability is the pre-processing of the training data. Some models react sensitively to certain characteristics of the predictor or response data. Those characteristics include, inter alia, distributional skewness and outliers and there exist several methods to lower their potentially bad impact on the model's performance. 

Since we simulated our training data under the null of non-cointegration we expect the distribution of this test statistic to be rather right skewed. \textcolor{red}{Plot} also reveals it to have a long right tail. If we train our regression model on this raw data it can possibly have difficulties predicting from high values of the test statistic. 

One of the aforementioned methods to deal with such issues are power transforms. One might decide freely which transformation to apply. Alternatively, there exist statistical methods to determine an appropriate transformation. A well-known family of transformations to un-skew data is the Box-Cox transformation [@Boxcox_1964]. They aim at transforming the data so that it closely resembles the normal distribution. The exact transformation depends on the parameter $\lambda$, whose optimal value can be empirically estimated: 

\begin{equation}
y^{(\lambda)} =
    \begin{cases}
    \frac{y^{\lambda} - 1}{\lambda}, & \lambda \neq 0 \\
    \log{(y)}, & \lambda = 0
    \end{cases}
\label{eq:6}
\end{equation}

It is visible from \eqref{eq:6} that @Boxcox_1964 developed these transformations for the dependent variable. @Kuhn_2013, however, report that it proves as effective for transforming individual regressors. We estimate lambda for the values of the test statistics of the Bayer-Hanck test and transform them according to \eqref{eq:5}. This forces their distribution into a more symmetric form, albeit still right-tailed. 

Since the response variable consists of the p-values, simulated under the null hypothesis, it follows a uniform distribution. Therefore, it is already symmetric. We still include a Box-Cox transformed and a logarithmised version of the response variable to see if it benefits the prediction. 

We also include various variations of the actual categorical variable $k$. It is firstly decomposed into dummy variables and secondly recode as a numeric, so that various transformations can be performed.

## Polynomial Regression
Due to the reasons given above we restrict ourselves to linear models. The empirical \ac{CDF}, which we aim to predict, is typically known to have a curved shape. For this reason, a simple linear regression model is very unlikely to provide a satisfactory fit to the data. We skip this in favor of a more flexible model. We stay with least squares regression, but try various combinations of polynomial functions and interaction terms of the aforementioned regressors. The search for the best model is carried out via brute-force.

Polynomial Regression extends the classic linear regression model by fitting a polynomial equation of arbitrary order to the data. A polynomial regression with $n$ degrees thus takes the form

\begin{equation}
    y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + ... + \beta_n x_i^n + \varepsilon_i,
\label{eq:7}
\end{equation}

where $\varepsilon_i$ is the error term. \textcolor{red}{Quelle?}

Here, we calculate orthogonal polynomials of the test statistic of the Bayer-Hanck Test, considering up to 15 degrees. We estimate the parameters with OLS. To potentially increase the predictive performance of our model we also add interaction terms and different transformations of the regressor $k$. \textcolor{red}{Appendix} lists all calculated models. Since there is no need to prevent overfitting we expect higher order polynomials to perform better, as they are highly flexible. These polynomials, however, tend to show a wiggly behaviour at the boundaries. This makes prediction for more extreme values of the test statistic a risky endeavour. We will address and fix this issue later on. 

## \ac{Lasso}  
As mentioned above our polynomial regression models are likely to perform best with higher order polynomials. With each added polynomial, however, we increase the complexity of our model and potentially add redundant regressors. Although, still, overfitting plays no major role here, we generally prefer sparser models in case of equal results. One way to deal with this is the use of variable selection methods. A well-known example of such methods is the \ac{Lasso}. 

The lasso estimate is defined as 

\begin{equation}
    \hat{\beta}^{\text{lasso}} = \argmin_{\beta} \sum^N_{i=1} \left( y_i - \beta_0 - \sum^p_{j=1} x_{ij} \beta_j \right)^2
    \text{s.t.} \sum^p_{j=1} |\beta_j | \leq t, 
\label{eq:8}
\end{equation}

where the first term describes the residual sum of squares, subject to a term known as L1 penalty. In its Lagrangian form this can be rewritten as

\begin{equation}
    \hat{\beta}^{\text{lasso}} = \argmin_{\beta} \frac{1}{2} \sum^N_{i=1} \left( y_i - \beta_0 - \sum^p_{j=1} x_{ij} \beta_j \right)^2 + 
    \lambda \sum^p_{j=1} |\beta_j |
\label{eq:9}
\end{equation}

$\lambda$ is a tuning parameter which defines the degree of regularisation. The lasso penalty shrinks the coefficients and, for $\lambda$ sufficiently large, can set them to zero. The value of $\lambda$ is data dependent and is usually estimated with cross-validation. \textcolor{red}{ausfÃ¼hrlicher? Quelle?}

We plan on fitting a LASSO model to polynomials of grade 15. We consider the same transformations and interaction terms as in earlier steps. We therefore fit a total of \textcolor{red}{Anzahl} models.

## Other Regression Models 
We also considered various other regression models. For different reasons they were not too suitable for our particular case. Conventional non-linear methods, like Generalized Additive Models or Multivariate Adaptive Regression Splines, might have provided a decent prediction. However, the fitted models take up far too much memory space compared to the aforementioned linear methods. For the same reason we refrain from using tree based methods. Add to that the fact that the latter tend to perform poorly with such a small amount of regressors. Given these limitations, we stick to our decision to solely work with linear regression models.


# Model Evaluation
We estimate all models for two different combinations of the underlying tests. Namely, these are a combination of the Engle-Granger and Johansen test (EJ) and a combination of all four underlying tests (all). Furthermore, we estimate one model per specification of the model deterministics. Altogether, this results in a total of six different models. 

## RMSE comparison
To measure the performance of our regression models we calculate their in-sample RMSE. This is an indication of how far the residuals of the models are from zero, with lower values preferable. We calculate the RMSE for predictions on the full distribution, as well as predictions on the lower tail ($p \leq 0.2$), as it is more important for the test decision of the Bayer-Hanck test. We also add a corrected version of the RMSE, cRMSE, where predictions are corrected to be within the limited interval [0, 1].

Table \ref{tab:all_1} lists all variations of the RMSE for the calculated polynomial regression models. It becomes apparent that a combination of higher order polynomials, dummy variables and interaction terms indeed achieves superior results compared to simpler models. For all variations of the RMSE the best models require a polynomial of minimum grade 12. That was to be expected, considering we are optimising an in-sample fit. The transformation of the response variable only seems to play a minor role in prediction accuracy. Interestingly, there are no major differences in model selection depending on the variation of the RMSE used. Table \ref{tab:5_best_all_1} lists the five best models for each case and test type. 

For the above-mentioned reasons we choose the final models according to the cRMSE on the left tail of the distribution of the p-values. \textcolor{red}{Grafik mit den 6 final models}. It is apparent that the functional forms look very similar over all cases, mostly using the highest order polynomial available^[We are well aware of the fact that this represents a corner solution. Since we are optimising the models on the in-sample RMSE, however, we could continue adding higher order polynomials forever to improve the fit. As we are already tweaking on the fifth decimal place we decided to not further pursue this procedure.]. Furthermore, five out of six models use the Box-Cox transformed response variable. 

## Correction for high values of the test statistic

As described in [chapter 3](#Simulation) the data set used for training the models was simulated under the null hypothesis of no cointegration. It should be evident that for this reason most values of the test statistic will be comparatively small. Even after its transformation the distribution of the test statistic has a longer right tail, i.e. there exist few high values. When using the models within a software package, as originally intended, it is likely that they will face input values located on the far right of the central part of the distribution. It cannot be ruled out that the models will fail to make sensible predictions for such values of the test statistic. 

\ref{fig:fig_3} shows the prediction of the final models for all underlying tests on a sequence from 1 to 100, representing possible values of the test statistic. Surprisingly, in the majority of cases the models perform well, with the prediction line taking the expected shape. In two cases, however, the predicted values rise again, taking values not equal to zero. More precisely, this occurs for the model with no deterministics (Case = 1) and $k = 3$ and $k = 4$, respectively.

\ref{fig:fig_4} shows the same behaviour for the data with Engle-Granger and Johansen as underlying tests for all combinations of cases and $k$. Above a certain value of the test statistic the predicted values sharply increase, converging \textcolor{red}{(?)} against 1. It should be noted that this upper boundary is enforced by our build in correction for predicted values outside the interval [0, 1]. Without this intervention the predicted values would probably rise even further. If we predict on an extended sequence with no correction, the prediction line most likely oscillates above a certain value. 

It cannot be clearly established why the models' prediction behaves this way. Oscillation at the edges of an interval is a common problem in polynomial interpolation, especially when using polynomials of high order. Additionally, the distribution of the test statistic in the training data may have made matters worse. It must also be considered that we chose our models according to their predictive performance on the lower tail of the distribution, possibly neglecting the predictive performance on the upper tail. If this incident is not rectified the models will be unable to provide reliable test decisions, as they tend to falsely not reject the null hypothesis at high values of the test statistic. The approach is therefore prone to type II errors.

From this it appears that there is a need for either more reliable models or further correction of the prediction from the existing models. One possible solution might be the re-estimation of our models using splines. In theory, those lead to similar results while being less prone to oscillation at the tails. Another approach is to determine a critical value of the test statistic whereby every exceeding value is automatically assigned a low p-value. Due to the former approach being very time-consuming we decided to try the last one. 

To find a suitable value of the test statistic, which we can use as the critical value, we observe \textcolor{red}{(Grafik)}. Regarding the curve of the predicted p-values it could be an attempt at a solution to take the value of the test statistic with the lowest p-value and use this as our critical value. We do this for all combinations of the test type, cases and $k$s. The resulting values range between 36 and 44 for the EJ combination and 55 and 80 for all combinations respectively. It is safe to say that these values lie far from the lower tail of the test statistics where the test decision is taken. Graphically it can also be assumed that our approach suffices for our purposes without any unintended side-effects. Thus, the problem of erratic behaviour for higher values of the test statistic is avoided.


# Package

## Algorithmic implementation

Not part of this chapter on package implementation will be the calculation of the underlying tests and Bayerhanck statistics, as these are already available. The approximation of the $p$-value is implemented by means of a separate function named `get_p_value`, which takes the arguments `bh.test`, `trendtype`, `test.type`, and `k`. With `bh.test` specifying the test statistic, `trendtype` the case, `test.type` which underlying test were used and `k` the number of regressors. The first step within the function is to check whether the calculated test statistic is greater than the critical value, given the configuration of the test and the parameters. When this is the case, a deterministic $p$-value of $<1e-12$ is always predicted and the rest of the approximation process is skipped. However, if this is not the case -- i.e., the test statistic is less than the critical value -- the actual approximation of the function is now performed. 

All required data and models are organized in the tibble `models`, which contains the `test.type`, the `trendtype`, the `models`, the $\lambda$ value for the box-cox transformation for the test statistic and the $p$-value, and the `critical value`. In order to perform the approximation, we first need the $\lambda$ parameters for the two transformation of the test statistic and the $p$-value for the corresponding configuration of the test performed. In the next step the corresponding model is selected. Before we can perform the estimation we have to merge the passed variables, e.g. `bh.test` and `k`, into a data frame and then additionally include them as a box-cox transformation.

Now we can do the p-value approximation using the appropriate model. If the response variable was box-cox transformed in the model, the variable is transformed back in the next step. The last step in the algorithm is to keep the estimated p-value within the theoretical limits of 0 and 1. If a p-value of 0 or a negative value is predicted, it is transformed to $< 1e-12$ as in the correction before. For values above 1, it is set to $9.9999e-1$.   



# Conclusion

<!-- end of main part -->

\pagebreak

\pagenumbering{Roman}
\setcounter{page}{3}
\addcontentsline{toc}{section}{References}
\printbibliography[title = References]
\cleardoublepage

\begin{refsection}
\nocite{R-base}
\nocite{R-stargazer}
\nocite{R-stringr}
\nocite{R-tidyr}
\nocite{R-dplyr}
\nocite{R-glmnet}
\nocite{R-class}
\nocite{R-MASS}
\nocite{R-plm}
\nocite{R-leaps}
\nocite{R-caret}
\nocite{R-tree}
\nocite{R-gbm}
\nocite{R-plotmo}
\nocite{R-pls}
\nocite{R-splines}
\nocite{R-tictoc}
\nocite{R-plotly}
\nocite{R-inspectdf}
\nocite{R-rpart}
\nocite{R-rpart.plot}
\nocite{R-stargazer}
\nocite{R-knitr}
\nocite{R-purrr}
\nocite{R-randomForest}
\nocite{R-rstudioapi}





\nocite{R-Studio}

\printbibliography[title = Software-References]
\addcontentsline{toc}{section}{Software-References}
\end{refsection}


<!---
--------------------------------------------------------------------------------
------------- Appendix ---------------------------------------------------------
--------------------------------------------------------------------------------
-->

\cleardoublepage
\appendix
\setcounter{table}{0}
\setcounter{figure}{0}
\renewcommand{\thetable}{A\arabic{table}}
\renewcommand{\thefigure}{A\arabic{figure}}

\newgeometry{top = 2.5cm, left = 2.5cm, right = 2cm, bottom = 2cm}

# Appendices

Table \ref{tab:func_form} list the different functional forms of the polynomial regression we tested. In total we investigated $21$ different forms and for each of these forms we investigated the polynomial in the range from $3$ to $13$. As equations with many polynomials are getting very long we will use a short-hand notation. For example the first equation in Table \ref{tab:func_form} for a polynomial of $3$ is in short-hand notation

\begin{align}
    p = c + \poly\left( \bc(t), 3 \right)
\end{align}

and represents

\begin{align}
    p = c + \gamma_{1,1} t + \gamma_{1,2} t^2 + \gamma_{1,1} t^3 .
\end{align}

\begin{table}
	\centering
	\caption{Description of all tested functional forms for polynomial regression. All functional forms were tested for a maximum polynomial degree from 3 to 13. The shorthand notation was used for the description.}
	\label{tab:func_form}	 
	\begin{tabular}{rlc}
		Number & Functional form & Range of $\gamma$ \\
		\toprule
		$1$ & $p = c + \poly\left( \bc(t), \gamma \right) $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\ 
		$2$ & $p = c + \poly\left( \bc(t), \gamma \right) + k $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\
		$3$ & $p = c + \poly\left( \bc(t), \gamma \right) * k $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\
		$4$ & $p = c + \poly\left( \bc(t), \gamma \right) + \log(k) $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\
		$5$ & $p = c + \poly\left( \bc(t), \gamma \right) * \log(k) $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\
		$6$ & $p = c + \poly\left( \bc(t), \gamma \right) + k\_d $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\
		$7$ & $p = c + \poly\left( \bc(t), \gamma \right) * k\_d $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\ %[0.5em]
		\midrule
		$8$ & $\log(p) = c + \poly\left( \bc(t), \gamma \right) $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\ 
		$9$ & $\log(p) = c + \poly\left( \bc(t), \gamma \right) + k $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\
		$10$ & $\log(p) = c + \poly\left( \bc(t), \gamma \right) * k $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\
		$11$ & $\log(p) = c + \poly\left( \bc(t), \gamma \right) + \log(k) $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\
		$12$ & $\log(p) = c + \poly\left( \bc(t), \gamma \right) * \log(k) $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\
		$13$ & $\log(p) = c + \poly\left( \bc(t), \gamma \right) + k\_d $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\
		$14$ & $\log(p) = c + \poly\left( \bc(t), \gamma \right) * k\_d $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\	
		\midrule
		$15$ & $\bc(p) = c + \poly\left( \bc(t), \gamma \right) $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\ 
		$16$ & $\bc(p) = c + \poly\left( \bc(t), \gamma \right) + k $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\
		$17$ & $\bc(p) = c + \poly\left( \bc(t), \gamma \right) * k $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\
		$18$ & $\bc(p) = c + \poly\left( \bc(t), \gamma \right) + \log(k) $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\
		$19$ & $\bc(p) = c + \poly\left( \bc(t), \gamma \right) * \log(k) $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\
		$20$ & $\bc(p) = c + \poly\left( \bc(t), \gamma \right) + k\_d $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\
		$21$ & $\bc(p) = c + \poly\left( \bc(t), \gamma \right) * k\_d $ & $\gamma \in \mathbb{Z} \left[3, 13 \right]$\\	
		\bottomrule
	\end{tabular}
\end{table}
\FloatBarrier

## Results for the $p$-approximation of the Bayer-Hanck Test with all underyling Tests

### Metrics of the 5 Best Models

```{r 5_best_all_1, echo = FALSE, warning = FALSE, message = FALSE}

table_all_case_1 %>%
    best_5_table_paper() %>%
    dplyr::mutate_if(is.numeric, funs(as.character(format(., scientific = TRUE, digits = 3)))) %>%
    knitr::kable(booktabs = TRUE, linesep = "", col.names = c('Model', 'RMSE', 'cRMSE', 'RMSE', 'cRMSE' ), 
             row.names = TRUE, escape = FALSE,
              caption= '\\label{tab:best_all_1} The five best models, based on the cRMSE for the lower tail of the distribution, for the first case (no constant, no trend) and all underlying tests included. The RMSE and cRMSE were calculated over the whole distribution and over the lower tail of the distribution. The cRMSE reflects the RMSE after correcting for values ranging between 0 and 1.', align = 'lrrrr') %>%
    column_spec(3:6, width = "2cm") %>%
  kable_styling(latex_options = c("striped", "hold_position"), font_size = 10) %>%
  add_header_above(c("", "", "Full Distribution" = 2, "Lower Tail ($p \\\\leq 0.2$)" = 2),  bold = TRUE, escape = FALSE)
```

```{r 5_best_all_2, echo = FALSE, warning = FALSE, message = FALSE}

table_all_case_2 %>%
    best_5_table_paper() %>%
    dplyr::mutate_if(is.numeric, funs(as.character(format(., scientific = TRUE, digits = 3)))) %>%
    knitr::kable(booktabs = TRUE, linesep = "", col.names = c('Model', 'RMSE', 'cRMSE', 'RMSE', 'cRMSE' ), 
             row.names = TRUE, escape = FALSE,
              caption= '\\label{tab:best_all_2} The five best models, based on the cRMSE for the lower tail of the distribution, for the second case (with constant, no trend) and all underlying tests included. The RMSE and cRMSE were calculated over the whole distribution and over the lower tail of the distribution. The cRMSE reflects the RMSE after correcting for values ranging between 0 and 1.', align = 'lrrrr') %>%
    column_spec(3:6, width = "2cm") %>%
  kable_styling(latex_options = c("striped", "hold_position"), font_size = 10) %>%
  add_header_above(c("", "", "Full Distribution" = 2, "Lower Tail ($p \\\\leq 0.2$)" = 2),  bold = TRUE, escape = FALSE)
```

```{r 5_best_all_3, echo = FALSE, warning = FALSE, message = FALSE}

table_all_case_3 %>%
    best_5_table_paper() %>%
    dplyr::mutate_if(is.numeric, funs(as.character(format(., scientific = TRUE, digits = 3)))) %>%
    knitr::kable(booktabs = TRUE, linesep = "", col.names = c('Model', 'RMSE', 'cRMSE', 'RMSE', 'cRMSE' ), 
             row.names = TRUE, escape = FALSE,
              caption= '\\label{tab:best_all_3} The five best models, based on the cRMSE for the lower tail of the distribution, for the third case (with constant and trend) and all underlying tests included. The RMSE and cRMSE were calculated over the whole distribution and over the lower tail of the distribution. The cRMSE reflects the RMSE after correcting for values ranging between 0 and 1.', align = 'lrrrr') %>%
    column_spec(3:6, width = "2cm") %>%
  kable_styling(latex_options = c("striped", "hold_position"), font_size = 10) %>%
  add_header_above(c("", "", "Full Distribution" = 2, "Lower Tail ($p \\\\leq 0.2$)" = 2),  bold = TRUE, escape = FALSE)
```

\FloatBarrier
### Metrics of all Models
```{r all_1, echo = FALSE, warning = FALSE, message = FALSE}
table_all_case_1 %>%
    dplyr::select(-c(formula, expo, model)) %>%
    dplyr::mutate_if(is.numeric, funs(as.character(format(., scientific = TRUE, digits = 3)))) %>%
    knitr::kable(booktabs = TRUE, linesep = "", col.names = c('Model', 'RMSE', 'cRMSE', 'RMSE', 'cRMSE' ), 
             row.names = TRUE, escape = FALSE,
              caption= '\\label{tab:all_1} Performance of the models for the first case and all underlying tests included. The RMSE and cRMSE were calculated over the whole distribution and over the lower tail of the distribution. The cRMSE reflects the RMSE after correcting for values ranging between 0 and 1.', align = 'lrrrr', longtable = TRUE) %>%
    column_spec(3:6, width = "2cm") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), font_size = 10) %>%
  add_header_above(c("", "", "Full Distribution" = 2, "Lower Tail ($p \\\\leq 0.2$)" = 2),  bold = TRUE, escape = FALSE)
```


```{r all_2, echo = FALSE, warning = FALSE, message = FALSE}
table_all_case_2 %>%
    dplyr::select(-c(formula, expo, model)) %>%
    dplyr::mutate_if(is.numeric, funs(as.character(format(., scientific = TRUE, digits = 3)))) %>%
    knitr::kable(booktabs = TRUE, linesep = "", col.names = c('Model', 'RMSE', 'cRMSE', 'RMSE', 'cRMSE' ), 
             row.names = TRUE, escape = FALSE,
              caption= '\\label{tab:all_2} Performance of the models for the second case and all underlying tests included. The RMSE and cRMSE were calculated over the whole distribution and over the lower tail of the distribution. The cRMSE reflects the RMSE after correcting for values ranging between 0 and 1.', align = 'lrrrr', longtable = TRUE) %>%
    column_spec(3:6, width = "2cm") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), font_size = 10) %>%
  add_header_above(c("", "", "Full Distribution" = 2, "Lower Tail ($p \\\\leq 0.2$)" = 2),  bold = TRUE, escape = FALSE)
```

```{r all_3, echo = FALSE, warning = FALSE, message = FALSE}
table_all_case_3 %>%
    dplyr::select(-c(formula, expo, model)) %>%
    dplyr::mutate_if(is.numeric, funs(as.character(format(., scientific = TRUE, digits = 3)))) %>%
    knitr::kable(booktabs = TRUE, linesep = "", col.names = c('Model', 'RMSE', 'cRMSE', 'RMSE', 'cRMSE' ), 
             row.names = TRUE, escape = FALSE,
              caption= '\\label{tab:all_3} Performance of the models for the third case and all underlying tests included. The RMSE and cRMSE were calculated over the whole distribution and over the lower tail of the distribution. The cRMSE reflects the RMSE after correcting for values ranging between 0 and 1.', align = 'lrrrr', longtable = TRUE) %>%
    column_spec(3:6, width = "2cm") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), font_size = 10) %>%
  add_header_above(c("", "", "Full Distribution" = 2, "Lower Tail ($p \\\\leq 0.2$)" = 2),  bold = TRUE, escape = FALSE)
```

## Results for the $p$-approximation of the Bayer-Hanck Test with Engle-Granger and Johansen as underlying tests

### Metrics of the 5 Best Models

```{r 5_best_e_j_1, echo = FALSE, warning = FALSE, message = FALSE}
table_E_J_case_1 %>%
    best_5_table_paper() %>%
    dplyr::mutate_if(is.numeric, funs(as.character(format(., scientific = TRUE, digits = 3)))) %>%
    knitr::kable(booktabs = TRUE, linesep = "", col.names = c('Model', 'RMSE', 'cRMSE', 'RMSE', 'cRMSE' ), 
             row.names = TRUE, escape = FALSE,
              caption= '\\label{tab:best_e_j_1} The five best models, based on the cRMSE for the lower tail of the distribution, for the first case (no constant, no trend) with Engle-Granger and Johansen as underlying tests. The RMSE and cRMSE were calculated over the whole distribution and over the lower tail of the distribution. The cRMSE reflects the RMSE after correcting for values ranging between 0 and 1.', align = 'lrrrr') %>%
    column_spec(3:6, width = "2cm") %>%
  kable_styling(latex_options = c("striped", "hold_position"), font_size = 10) %>%
  add_header_above(c("", "", "Full Distribution" = 2, "Lower Tail ($p \\\\leq 0.2$)" = 2),  bold = TRUE, escape = FALSE)
```

```{r 5_best_e_j_2, echo = FALSE, warning = FALSE, message = FALSE}
table_E_J_case_2 %>%
    best_5_table_paper() %>%
    dplyr::mutate_if(is.numeric, funs(as.character(format(., scientific = TRUE, digits = 3)))) %>%
    knitr::kable(booktabs = TRUE, linesep = "", col.names = c('Model', 'RMSE', 'cRMSE', 'RMSE', 'cRMSE' ), 
             row.names = TRUE, escape = FALSE,
              caption= '\\label{tab:best_e_j_2} The five best models, based on the cRMSE for the lower tail of the distribution, for the second case (with constant, no trend) with Engle-Granger and Johansen as underlying tests. The RMSE and cRMSE were calculated over the whole distribution and over the lower tail of the distribution. The cRMSE reflects the RMSE after correcting for values ranging between 0 and 1.', align = 'lrrrr') %>%
    column_spec(3:6, width = "2cm") %>%
  kable_styling(latex_options = c("striped", "hold_position"), font_size = 10) %>%
  add_header_above(c("", "", "Full Distribution" = 2, "Lower Tail ($p \\\\leq 0.2$)" = 2),  bold = TRUE, escape = FALSE)
```

```{r 5_best_e_j_3, echo = FALSE, warning = FALSE, message = FALSE}
table_E_J_case_3 %>%
    best_5_table_paper() %>%
    dplyr::mutate_if(is.numeric, funs(as.character(format(., scientific = TRUE, digits = 3)))) %>%
    knitr::kable(booktabs = TRUE, linesep = "", col.names = c('Model', 'RMSE', 'cRMSE', 'RMSE', 'cRMSE' ), 
             row.names = TRUE, escape = FALSE,
              caption= '\\label{tab:best_e_j_3} The five best models, based on the cRMSE for the lower tail of the distribution, for the third case (with constant and trend) with Engle-Granger and Johansen as underlying tests. The RMSE and cRMSE were calculated over the whole distribution and over the lower tail of the distribution. The cRMSE reflects the RMSE after correcting for values ranging between 0 and 1.', align = 'lrrrr') %>%
    column_spec(3:6, width = "2cm") %>%
  kable_styling(latex_options = c("striped", "hold_position"), font_size = 10) %>%
  add_header_above(c("", "", "Full Distribution" = 2, "Lower Tail ($p \\\\leq 0.2$)" = 2),  bold = TRUE, escape = FALSE)
```

\FloatBarrier

### Metrics of all Models
```{r e_j_1, echo = FALSE, warning = FALSE, message = FALSE}
table_E_J_case_1 %>%
    dplyr::select(-c(formula, expo, model)) %>%
    dplyr::mutate_if(is.numeric, funs(as.character(format(., scientific = TRUE, digits = 3)))) %>%
    knitr::kable(booktabs = TRUE, linesep = "", col.names = c('Model', 'RMSE', 'cRMSE', 'RMSE', 'cRMSE' ), 
             row.names = TRUE, escape = FALSE,
              caption= '\\label{tab:e_j_1} Performance of the models for the first case with Engle-Granger and Johansen as underlying tests. The RMSE and cRMSE were calculated over the whole distribution and over the lower tail of the distribution. The cRMSE reflects the RMSE after correcting for values ranging between 0 and 1.', align = 'lrrrr', longtable = TRUE) %>%
    column_spec(3:6, width = "2cm") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), font_size = 10) %>%
  add_header_above(c("", "", "Full Distribution" = 2, "Lower Tail ($p \\\\leq 0.2$)" = 2),  bold = TRUE, escape = FALSE)
```

```{r e_j_2, echo = FALSE, warning = FALSE, message = FALSE}
table_E_J_case_2 %>%
    dplyr::select(-c(formula, expo, model)) %>%
    dplyr::mutate_if(is.numeric, funs(as.character(format(., scientific = TRUE, digits = 3)))) %>%
    knitr::kable(booktabs = TRUE, linesep = "", col.names = c('Model', 'RMSE', 'cRMSE', 'RMSE', 'cRMSE' ), 
             row.names = TRUE, escape = FALSE,
              caption= '\\label{tab:e_j_2} Performance of the models for the second case with Engle-Granger and Johansen as underlying tests. The RMSE and cRMSE were calculated over the whole distribution and over the lower tail of the distribution. The cRMSE reflects the RMSE after correcting for values ranging between 0 and 1.', align = 'lrrrr', longtable = TRUE) %>%
    column_spec(3:6, width = "2cm") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), font_size = 10) %>%
  add_header_above(c("", "", "Full Distribution" = 2, "Lower Tail ($p \\\\leq 0.2$)" = 2),  bold = TRUE, escape = FALSE)
```

```{r e_j_3, echo = FALSE, warning = FALSE, message = FALSE}
table_E_J_case_3 %>%
    dplyr::select(-c(formula, expo, model)) %>%
    dplyr::mutate_if(is.numeric, funs(as.character(format(., scientific = TRUE, digits = 3)))) %>%
    knitr::kable(booktabs = TRUE, linesep = "", col.names = c('Model', 'RMSE', 'cRMSE', 'RMSE', 'cRMSE' ), 
             row.names = TRUE, escape = FALSE,
              caption= '\\label{tab:e_j_3} Performance of the models for the second case with Engle-Granger and Johansen as underlying tests. The RMSE and cRMSE were calculated over the whole distribution and over the lower tail of the distribution. The cRMSE reflects the RMSE after correcting for values ranging between 0 and 1.', align = 'lrrrr', longtable = TRUE) %>%
    column_spec(3:6, width = "2cm") %>%
  kable_styling(latex_options = c("striped", "hold_position", "repeat_header"), font_size = 10) %>%
  add_header_above(c("", "", "Full Distribution" = 2, "Lower Tail ($p \\\\leq 0.2$)" = 2),  bold = TRUE, escape = FALSE)
```

\FloatBarrier





```{r , approx_sim-all, echo = FALSE, fig.cap = " \\label{fig:sim_approx_all} Simulated against approximated $p$-values over the whole distribution for all cases and all underlying tests. ", fig.height = 8}
p.sim_p.aprox_all
```

```{r , approx_sim-all_0.2, echo = FALSE, fig.cap = " \\label{fig:sim_approx_all_0.2} Simulated vs. approximated $p$-values for the lower tail of the distribution for all cases and all underlying test.", fig.height = 8}
p.sim_p.aprox_all_0.2
```

```{r , p_stat_all, echo = FALSE, fig.cap = " \\label{fig:fig_3} Corrected (blue) and uncorrected (red) $p$-value predictions for all cases and all underlying tests.", fig.height = 8}
plot_p_stat_all
```

```{r , p_stat_e_j, echo = FALSE, fig.cap = " \\label{fig:fig_4} Corrected (blue) and uncorrected (red) $p$-value predictions for all cases using Engle-Granger and Johansen as underlying tests.", fig.height = 8}
plot_p_stat_e_j
```



<!-- 
--------------------------------------------------------------------------------
------------- End of Appendix --------------------------------------------------
--------------------------------------------------------------------------------
--> 
\restoregeometry

\cleardoublepage


