
@article{smith_nonparametric_1996,
	title = {Nonparametric regression using {Bayesian} variable selection},
	volume = {75},
	issn = {03044076},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0304407695017631},
	doi = {10.1016/0304-4076(95)01763-1},
	abstract = {This paper estimates an additive model semiparametrically, while automatically selecting the significant independent variables and the app{\textasciitilde}opriatc power transformation of the dependent variable. The nonlinear variables arc modeled as regression splincs, with significant knots selected fiom a large number of candidate knots. The estimation is made robust by modeling the errors as a mixture of normals. A Bayesian approach is used to select the significant knots, the power transformation, and to identify oatliers using the Gibbs sampler to curry out the computation. Empirical evidence is given that the sampler works well on both simulated and real examples and that in the univariate case it compares faw)rably with a kernel-weighted local linear smoother, The variable selection algorithm in the paper is substantially fasler than previous Bayesian variable sclcclion algorithms.},
	language = {en},
	number = {2},
	urldate = {2019-12-07},
	journal = {Journal of Econometrics},
	author = {Smith, Michael and Kohn, Robert},
	month = dec,
	year = {1996},
	pages = {317--343},
	file = {Smith und Kohn - 1996 - Nonparametric regression using Bayesian variable s.pdf:C\:\\Users\\HP\\Zotero\\storage\\7NQHVLU2\\Smith und Kohn - 1996 - Nonparametric regression using Bayesian variable s.pdf:application/pdf}
}

@article{tibshirani_regression_1996,
	title = {Regression {Shrinkage} and {Selection} via the {Lasso}},
	volume = {58},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2346178},
	abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
	number = {1},
	urldate = {2020-01-06},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Tibshirani, Robert},
	year = {1996},
	pages = {267--288}
}

@misc{leone_fifa_2020,
	title = {{FIFA} 20 complete player dataset},
	url = {https://kaggle.com/stefanoleone992/fifa-20-complete-player-dataset},
	abstract = {18k+ players, 100+ attributes extracted from the latest edition of FIFA},
	language = {en},
	urldate = {2020-01-07},
	author = {Leone, Stefano},
	year = {2020},
	file = {Snapshot:C\:\\Users\\HP\\Zotero\\storage\\MKKKDZ9S\\fifa-20-complete-player-dataset.html:text/html}
}

@article{park_bayesian_2008,
	title = {The {Bayesian} {Lasso}},
	volume = {103},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/016214508000000337},
	doi = {10.1198/016214508000000337},
	abstract = {The Lasso estimate for linear regression parameters can be interpreted as a Bayesian posterior mode estimate when the regression parameters have independent Laplace (i.e., double-exponential) priors. Gibbs sampling from this posterior is possible using an expanded hierarchy with conjugate normal priors for the regression parameters and independent exponential priors on their variances. A connection with the inverse-Gaussian distribution provides tractable full conditional distributions. The Bayesian Lasso provides interval estimates (Bayesian credible intervals) that can guide variable selection. Moreover, the structure of the hierarchical model provides both Bayesian and likelihood methods for selecting the Lasso parameter. Slight modifications lead to Bayesian versions of other Lasso-related estimation methods, including bridge regression and a robust variant.},
	number = {482},
	urldate = {2020-01-11},
	journal = {Journal of the American Statistical Association},
	author = {Park, Trevor and Casella, George},
	month = jun,
	year = {2008},
	keywords = {Empirical Bayes, Gibbs sampler, Hierarchical model, Inverse Gaussian, Linear regression, Penalized regression, Scale mixture of normals},
	pages = {681--686},
	file = {Full Text PDF:C\:\\Users\\HP\\Zotero\\storage\\BJCDZPW8\\Park und Casella - 2008 - The Bayesian Lasso.pdf:application/pdf;Snapshot:C\:\\Users\\HP\\Zotero\\storage\\IKK736KJ\\016214508000000337.html:text/html}
}

@book{ghosh_introduction_2006,
	address = {New York},
	series = {Springer {Texts} in {Statistics}},
	title = {An {Introduction} to {Bayesian} {Analysis}: {Theory} and {Methods}},
	isbn = {978-0-387-40084-6},
	shorttitle = {An {Introduction} to {Bayesian} {Analysis}},
	url = {https://www.springer.com/de/book/9780387400846},
	abstract = {This is a graduate-level textbook on Bayesian analysis blending modern Bayesian theory, methods, and applications. Starting from basic statistics, undergraduate calculus and linear algebra, ideas of both subjective and objective Bayesian analysis are developed to a level where real-life data can be analyzed using the current techniques of statistical computing. Advances in both low-dimensional and high-dimensional problems are covered, as well as important topics such as empirical Bayes and hierarchical Bayes methods and Markov chain Monte Carlo (MCMC) techniques. Many topics are at the cutting edge of statistical research. Solutions to common inference problems appear throughout the text along with discussion of what prior to choose. There is a discussion of elicitation of a subjective prior as well as the motivation, applicability, and limitations of objective priors. By way of important applications the book presents microarrays, nonparametric regression via wavelets as well as DMA mixtures of normals, and spatial analysis with illustrations using simulated and real data. Theoretical topics at the cutting edge include high-dimensional model selection and Intrinsic Bayes Factors, which the authors have successfully applied to geological mapping. The style is informal but clear. Asymptotics is used to supplement simulation or understand some aspects of the posterior. J.K. Ghosh has been Director and Jawaharlal Nehru Professor at the Indian Statistical Institute and President of the International Statistical Institute. He is currently a professor of statistics at Purdue University and professor emeritus at the Indian Statistical Institute. He has been the editor of Sankhya and has served on the editorial boards of several journals including the Annals of Statistics. His current interests in Bayesian analysis include asymptotics, nonparametric methods, high-dimensional model selection, reliability and survival analysis, bioinformatics, astrostatistics and sparse and not so sparse mixtures. Mohan Delampady and Tapas Samanta are both professors of statistics at the Indian Statistical Institute and both are interested in Bayesian inference, specifically in topics such as model selection, asymptotics, robustness and nonparametrics.},
	language = {en},
	urldate = {2020-01-11},
	publisher = {Springer-Verlag},
	author = {Ghosh, Jayanta K. and Delampady, Mohan and Samanta, Tapas},
	year = {2006},
	doi = {10.1007/978-0-387-35433-0},
	file = {Eingereichte Version:C\:\\Users\\HP\\Zotero\\storage\\D7XM4ASD\\Ghosh et al. - 2006 - An Introduction to Bayesian Analysis Theory and M.pdf:application/pdf;Snapshot:C\:\\Users\\HP\\Zotero\\storage\\XBIM6VCN\\9780387400846.html:text/html}
}

@article{andrews_scale_1974,
	title = {Scale {Mixtures} of {Normal} {Distributions}},
	volume = {36},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2984774},
	abstract = {This paper presents necessary and sufficient conditions under which a random variable X may be generated as the ratio Z/V where Z and V are independent and Z has a standard normal distribution. This representation is useful in Monte Carlo calculations. It is established that when 1/2V$^{\textrm{2}}$ is exponential, X is double exponential; and that when 1/2V has the asymptotic distribution of the Kolmogorov distance statistic, X is logistic.},
	number = {1},
	urldate = {2020-01-11},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Andrews, D. F. and Mallows, C. L.},
	year = {1974},
	pages = {99--102}
}

@book{gelman_bayesian_2004,
	address = {Boca Raton [u.a.]},
	edition = {2. ed.},
	title = {Bayesian data analysis},
	isbn = {978-1-58488-388-3},
	language = {eng},
	publisher = {Chapman \& Hall/CRC},
	author = {Gelman, Andrew},
	year = {2004},
	keywords = {Bayes-Verfahren}
}

@misc{gramacy_monomvn_2019,
	title = {monomvn: {Estimation} for {MVN} and {Student}-t {Data} with {Monotone} {Missingness}},
	copyright = {LGPL-2 {\textbar} LGPL-2.1 {\textbar} LGPL-3 [expanded from: LGPL]},
	shorttitle = {monomvn},
	url = {https://CRAN.R-project.org/package=monomvn},
	abstract = {Estimation of multivariate normal (MVN) and student-t data of arbitrary dimension where the pattern of missing data is monotone. See Pantaleo and Gramacy (2010) {\textless}doi:10.1214/10-BA602{\textgreater}. Through the use of parsimonious/shrinkage regressions (plsr, pcr, lasso, ridge, etc.), where standard regressions fail, the package can handle a nearly arbitrary amount of missing data. The current version supports maximum likelihood inference and a full Bayesian approach employing scale-mixtures for Gibbs sampling. Monotone data augmentation extends this Bayesian approach to arbitrary missingness patterns. A fully functional standalone interface to the Bayesian lasso (from Park \& Casella), Normal-Gamma (from Griffin \& Brown), Horseshoe (from Carvalho, Polson, \& Scott), and ridge regression with model selection via Reversible Jump, and student-t errors (from Geweke) is also provided.},
	urldate = {2020-01-12},
	author = {Gramacy, Robert B.},
	month = nov,
	year = {2019}
}

@misc{hastle_glmnet_2019,
	title = {glmnet: {Lasso} and {Elastic}-{Net} {Regularized} {Generalized} {Linear} {Models}},
	copyright = {LGPL-2 {\textbar} LGPL-2.1 {\textbar} LGPL-3 [expanded from: LGPL]},
	shorttitle = {glmnet},
	url = {https://CRAN.R-project.org/package=monomvn},
	abstract = {Extremely efficient procedures for fitting the entire lasso or elastic-net regularization path for linear regression, logistic and multinomial regression models, Poisson regression and the Cox model. Two recent additions are the multiple-response Gaussian, and the grouped multinomial regression. The algorithm uses cyclical coordinate descent in a path-wise fashion, as described in the papers listed in the URL below.},
	urldate = {2020-01-12},
	author = {Hastle, Trevor},
	month = nov,
	year = {2019}
}

@book{hayashi_econometrics_2000,
	address = {Princeton [u.a.]},
	title = {Econometrics},
	isbn = {978-0-691-01018-2},
	abstract = {FN Ausgabebez.: Hier auch spaeter erschienene, unveraenderte Nachdrucke},
	language = {eng},
	publisher = {Princeton UnivPress},
	author = {Hayashi, Fumio},
	year = {2000},
	keywords = {Lehrbuch, {\"O}konometrie}
}

@article{hans_bayesian_2009,
	title = {Bayesian lasso regression},
	volume = {96},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/27798870},
	abstract = {The lasso estimate for linear regression corresponds to a posterior mode when independent, double-exponential prior distributions are placed on the regression coefficients. This paper introduces new aspects of the broader Bayesian treatment of lasso regression. A direct characterization of the regression coefficients' posterior distribution is provided, and computation and inference under this characterization is shown to be straightforward. Emphasis is placed on point estimation using the posterior mean, which facilitates prediction of future observations via the posterior predictive distribution. It is shown that the standard lasso prediction method does not necessarily agree with model-based, Bayesian predictions. A new Gibbs sampler for Bayesian lasso regression is introduced.},
	number = {4},
	urldate = {2020-01-16},
	journal = {Biometrika},
	author = {Hans, Chris},
	year = {2009},
	pages = {835--845}
}

@article{royall_effect_1986,
	title = {The {Effect} of {Sample} {Size} on the {Meaning} of {Significance} {Tests}},
	volume = {40},
	issn = {0003-1305},
	url = {https://www.jstor.org/stable/2684616},
	doi = {10.2307/2684616},
	abstract = {Contradictory interpretations of how the meaning of a significance test depends on the sample size are examined.},
	number = {4},
	urldate = {2020-01-17},
	journal = {The American Statistician},
	author = {Royall, Richard M.},
	year = {1986},
	pages = {313--315},
	file = {JSTOR Full Text PDF:C\:\\Users\\HP\\Zotero\\storage\\ZPJRQUN3\\Royall - 1986 - The Effect of Sample Size on the Meaning of Signif.pdf:application/pdf}
}